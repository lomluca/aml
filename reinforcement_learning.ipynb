{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will learn how to create a reinforcmenet learning agent for self driving Mario Kart.\n",
    "Then, we will build a leaderboard to compare the algrithms we build.\n",
    "\n",
    "## Goals\n",
    "\n",
    "The main goals of implementing self driving Mario Kart are:\n",
    "- Understand the basics of Reinforcement Learning\n",
    "- Get familiar with Open AI Gym and Universe\n",
    "- Create an agent that is capable of learning by itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reinforcement Learning\n",
    "\n",
    "## 1.1. Introduction\n",
    "Those interested in the world of machine learning are aware of the capabilities of reinforcement-learning-based AI. The past few years have seen many breakthroughs using reinforcement learning (RL). The company DeepMind combined deep learning with reinforcement learning to achieve above-human results on a multitude of Atari games and, in March 2016, defeated Go champion Le Sedol four games to one. Though RL is currently excelling in many game environments, it is a novel way to solve problems that require optimal decisions and efficiency, and will surely play a part in machine intelligence to come.\n",
    "\n",
    "Reinforcement learning, explained simply, is a computational approach where an agent interacts with an environment by taking actions in which it tries to maximize an accumulated reward. \n",
    "Here is a simple graph (the agent-environement loop), from the book [Reinforcement Learning: An Introduction 2nd Edition](http://incompleteideas.net/sutton/book/the-book-2nd.html) :\n",
    "\n",
    "![RL](https://d3ansictanv2wj.cloudfront.net/image3-5f8cbb1fb6fb9132fef76b13b8687bfc.png)\n",
    "\n",
    "An agent in a current state (St) takes an action (At) to which the environment reacts and responds, returning a new state(St+1) and reward (Rt+1) to the agent. Given the updated state and reward, the agent chooses the next action, and the loop repeats until an environment is solved or terminated.\n",
    "\n",
    "Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through it’s own interactions with an environment. By combining the complex representations that deep neural networks can learn with the goal-driven learning of an RL agent, computers have accomplished some amazing feats, like [beating humans at over a dozen Atari games](https://deepmind.com/dqn), and [defeating the Go world champion](https://deepmind.com/alpha-go).\n",
    "\n",
    "Learning how to build these agents requires a bit of a change in thinking for anyone used to working in a supervised learning setting though. Gone is the ability to simply get the algorithm to pair certain stimuli with certain responses. Instead RL algorithms must enable the agent to learn the correct pairings itself through the use of observations, rewards, and actions. Since there is no longer a “True” correct action for an agent to take in any given circumstance that we can just tell it, things get a little tricky. In this post and those to follow, I will be walking through the creation and training of reinforcement learning agents. The agent and task will begin simple, so that the concepts are clear, and then work up to more complex task and environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. OpenAI\n",
    "\n",
    "OpenAI was founded in late 2015 as a non-profit with a mission to “build safe artificial general intelligence (AGI) and ensure AGI's benefits are as widely and evenly distributed as possible.” In addition to exploring many issues regarding AGI, one major contribution that OpenAI made to the machine learning world was developing both the Gym and Universe software platforms.\n",
    "\n",
    "### OpenAI Gym\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. You can use it from Python code, and soon from other languages.\n",
    "\n",
    "OpenAI Gym consists of two parts:\n",
    "- The gym open-source library: a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "- The OpenAI Gym service: a site and API allowing people to meaningfully compare performance of their trained agents.\n",
    "\n",
    "### Running our first agent\n",
    "Here's a bare minimum example of getting something running. This will run an instance of the CartPole-v0 environment for 1000 timesteps, rendering the environment at each step. You should see a window pop up rendering the classic cart-pole problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-08 08:10:05,463] Making new env: CartPole-v0\n",
      "[2018-06-08 08:10:06,024] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"./Videos/cartpole-no-reset.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"./Videos/cartpole-no-reset.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to see some other environments in action, try replacing CartPole-v0 above with something like MountainCar-v0, MsPacman-v0 (requires the Atari dependency), or Hopper-v1 (requires the MuJoCo dependencies). Environments all descend from the Env base class.\n",
    "\n",
    "Note that if you're missing any dependencies, you should get a helpful error message telling you what you're missing.  Installing a missing dependency is generally pretty simple. You'll also need a MuJoCo license for Hopper-v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observtations\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it'd probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment's step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "- observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "- reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "- done (boolean): whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "- info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic \"agent-environment loop\". Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "![agent-environment loop](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
    "\n",
    "The process gets started by calling reset, which returns an initial observation. So a more proper way of writing the previous code would be to respect the done flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-08 08:10:20,472] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01405212 -0.01805954 -0.02092665 -0.04118234]\n",
      "[ 0.01369093  0.17735615 -0.02175029 -0.34039367]\n",
      "[ 0.01723805 -0.01744969 -0.02855817 -0.05464812]\n",
      "[ 0.01688906  0.17806986 -0.02965113 -0.35620281]\n",
      "[ 0.02045045 -0.01661825 -0.03677519 -0.07301517]\n",
      "[ 0.02011809 -0.21119421 -0.03823549  0.20784197]\n",
      "[ 0.0158942  -0.40574915 -0.03407865  0.48822259]\n",
      "[ 0.00777922 -0.21016338 -0.0243142   0.18499709]\n",
      "[ 0.00357595 -0.40492916 -0.02061426  0.46991173]\n",
      "[-0.00452263 -0.20952217 -0.01121602  0.17080336]\n",
      "[-0.00871307 -0.01424149 -0.00779995 -0.12539667]\n",
      "[-0.0089979  -0.20925084 -0.01030789  0.16481528]\n",
      "[-0.01318292 -0.40422372 -0.00701158  0.45422861]\n",
      "[-0.02126739 -0.59924583  0.00207299  0.7446932 ]\n",
      "[-0.03325231 -0.40415255  0.01696685  0.45266336]\n",
      "[-0.04133536 -0.59951028  0.02602012  0.75064579]\n",
      "[-0.05332557 -0.40475667  0.04103304  0.46626332]\n",
      "[-0.0614207  -0.21023778  0.0503583   0.18679091]\n",
      "[-0.06562546 -0.01587114  0.05409412 -0.08959036]\n",
      "[-0.06594288 -0.21172505  0.05230231  0.2196567 ]\n",
      "[-0.07017738 -0.01738823  0.05669545 -0.05608004]\n",
      "[-0.07052515 -0.21327534  0.05557385  0.25393804]\n",
      "[-0.07479065 -0.40914495  0.06065261  0.56361917]\n",
      "[-0.08297355 -0.60506316  0.07192499  0.87477704]\n",
      "[-0.09507481 -0.80108534  0.08942053  1.18917782]\n",
      "[-0.11109652 -0.99724522  0.11320409  1.50849666]\n",
      "[-0.13104143 -0.80366291  0.14337402  1.25319276]\n",
      "[-0.14711468 -1.00030033  0.16843788  1.5871302 ]\n",
      "[-0.16712069 -1.19697606  0.20018048  1.92725563]\n",
      "Episode finished after 29 timesteps\n",
      "[-0.02373303 -0.03421494 -0.02817137  0.02085324]\n",
      "[-0.02441733  0.16129945 -0.0277543  -0.28058327]\n",
      "[-0.02119134  0.35680609 -0.03336597 -0.581889  ]\n",
      "[-0.01405522  0.16216713 -0.04500375 -0.29990083]\n",
      "[-0.01081187  0.3579007  -0.05100176 -0.60643055]\n",
      "[-0.00365386  0.16352759 -0.06313037 -0.33023802]\n",
      "[-3.83308780e-04  3.59488736e-01 -6.97351338e-02 -6.42142196e-01]\n",
      "[ 0.00680647  0.16540457 -0.08257798 -0.37220904]\n",
      "[ 0.01011456  0.36159661 -0.09002216 -0.68974463]\n",
      "[ 0.01734649  0.55784487 -0.10381705 -1.0093558 ]\n",
      "[ 0.02850339  0.36425025 -0.12400417 -0.75099481]\n",
      "[ 0.03578839  0.56084431 -0.13902406 -1.07998538]\n",
      "[ 0.04700528  0.36780413 -0.16062377 -0.83396042]\n",
      "[ 0.05436136  0.56471318 -0.17730298 -1.17254259]\n",
      "[ 0.06565562  0.3722825  -0.20075383 -0.94027606]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.0395382  -0.00490569 -0.01255     0.01188569]\n",
      "[ 0.03944008  0.19039398 -0.01231229 -0.28473034]\n",
      "[ 0.04324796 -0.00455023 -0.01800689  0.00404408]\n",
      "[ 0.04315696 -0.19940937 -0.01792601  0.29099166]\n",
      "[ 0.03916877 -0.39427119 -0.01210618  0.57796747]\n",
      "[ 0.03128335 -0.19898167 -0.00054683  0.28149553]\n",
      "[ 0.02730371 -0.39409582  0.00508308  0.57400594]\n",
      "[ 0.0194218  -0.1990455   0.0165632   0.28292866]\n",
      "[ 0.01544089 -0.39439974  0.02222177  0.58078917]\n",
      "[ 0.00755289 -0.5898259   0.03383756  0.88038871]\n",
      "[-0.00424363 -0.7853908   0.05144533  1.18351448]\n",
      "[-0.01995144 -0.5909727   0.07511562  0.90739141]\n",
      "[-0.0317709  -0.7870268   0.09326345  1.22270652]\n",
      "[-0.04751143 -0.9832182   0.11771758  1.54309442]\n",
      "[-0.0671758  -0.78969143  0.14857947  1.28934237]\n",
      "[-0.08296962 -0.59673841  0.17436631  1.04662348]\n",
      "[-0.09490439 -0.40430512  0.19529878  0.81335477]\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.00025543  0.02179428 -0.00859709  0.02755843]\n",
      "[ 0.00069132  0.21703845 -0.00804592 -0.26782453]\n",
      "[ 0.00503209  0.02203225 -0.01340241  0.02230984]\n",
      "[ 0.00547273  0.21734381 -0.01295621 -0.27457139]\n",
      "[ 0.00981961  0.4126482  -0.01844764 -0.57131241]\n",
      "[ 0.01807257  0.60802392 -0.02987389 -0.86974942]\n",
      "[ 0.03023305  0.80353925 -0.04726888 -1.17167317]\n",
      "[ 0.04630384  0.99924284 -0.07070234 -1.47879254]\n",
      "[ 0.06628869  0.80505168 -0.10027819 -1.20900272]\n",
      "[ 0.08238973  0.61135752 -0.12445825 -0.94935346]\n",
      "[ 0.09461688  0.80791523 -0.14344532 -1.27840581]\n",
      "[ 0.11077518  0.61488309 -0.16901343 -1.0338599 ]\n",
      "[ 0.12307284  0.81179996 -0.18969063 -1.37448017]\n",
      "Episode finished after 13 timesteps\n",
      "[ 0.0464827  -0.03678142  0.00394695 -0.03253284]\n",
      "[ 0.04574707 -0.23195975  0.00329629  0.26139278]\n",
      "[ 0.04110788 -0.036885    0.00852415 -0.03024864]\n",
      "[ 0.04037018  0.15811368  0.00791917 -0.32022997]\n",
      "[ 0.04353245 -0.03712015  0.00151457 -0.02506019]\n",
      "[ 0.04279005 -0.23226379  0.00101337  0.26810022]\n",
      "[ 0.03814477 -0.42740019  0.00637537  0.56110259]\n",
      "[ 0.02959677 -0.23236829  0.01759743  0.27043504]\n",
      "[ 0.0249494  -0.03750182  0.02300613 -0.01664604]\n",
      "[ 0.02419937 -0.23294602  0.02267321  0.28320589]\n",
      "[ 0.01954045 -0.42838391  0.02833732  0.58295272]\n",
      "[ 0.01097277 -0.62389116  0.03999638  0.88442592]\n",
      "[-0.00150506 -0.8195327   0.0576849   1.18940934]\n",
      "[-0.01789571 -1.01535291  0.08147308  1.49960104]\n",
      "[-0.03820277 -1.2113646   0.1114651   1.81656988]\n",
      "[-0.06243006 -1.01764528  0.1477965   1.56049711]\n",
      "[-0.08278297 -0.82456872  0.17900644  1.31733381]\n",
      "[-0.09927434 -0.63210451  0.20535312  1.08559782]\n",
      "Episode finished after 18 timesteps\n",
      "[0.0242751  0.00529202 0.00813985 0.01659864]\n",
      "[ 0.02438094  0.2002963   0.00847182 -0.27350499]\n",
      "[ 0.02838687  0.39529635  0.00300172 -0.56350389]\n",
      "[ 0.0362928   0.59037606 -0.00826836 -0.85523963]\n",
      "[ 0.04810032  0.7856097  -0.02537315 -1.15051096]\n",
      "[ 0.06381251  0.5908279  -0.04838337 -0.8658912 ]\n",
      "[ 0.07562907  0.39639665 -0.0657012  -0.58880479]\n",
      "[ 0.083557    0.20225327 -0.07747729 -0.31752028]\n",
      "[ 0.08760207  0.00831541 -0.0838277  -0.05024172]\n",
      "[ 0.08776838 -0.18551073 -0.08483253  0.21486007]\n",
      "[ 0.08405816 -0.37932389 -0.08053533  0.47962349]\n",
      "[ 0.07647168 -0.57322207 -0.07094286  0.74587626]\n",
      "[ 0.06500724 -0.37719664 -0.05602534  0.43173803]\n",
      "[ 0.05746331 -0.5714824  -0.04739057  0.70624657]\n",
      "[ 0.04603366 -0.37573701 -0.03326564  0.39903053]\n",
      "[ 0.03851892 -0.57037165 -0.02528503  0.68104267]\n",
      "[ 0.02711149 -0.37490781 -0.01166418  0.38050762]\n",
      "[ 0.01961333 -0.5698622  -0.00405403  0.66949013]\n",
      "[ 0.00821609 -0.37468412  0.00933578  0.37553353]\n",
      "[ 0.00072241 -0.179696    0.01684645  0.08580878]\n",
      "[-0.00287151 -0.37505533  0.01856262  0.38375882]\n",
      "[-0.01037262 -0.18020178  0.0262378   0.09698594]\n",
      "[-0.01397666  0.01453449  0.02817752 -0.18730489]\n",
      "[-0.01368597 -0.18097904  0.02443142  0.11413207]\n",
      "[-0.01730555  0.01378448  0.02671406 -0.17074389]\n",
      "[-0.01702986 -0.18170945  0.02329918  0.13024536]\n",
      "[-0.02066405 -0.37715727  0.02590409  0.43018687]\n",
      "[-0.02820719 -0.57263629  0.03450783  0.73092197]\n",
      "[-0.03965992 -0.37800781  0.04912627  0.44929634]\n",
      "[-0.04722007 -0.18361389  0.05811219  0.17249458]\n",
      "[-0.05089235 -0.37951734  0.06156209  0.48292909]\n",
      "[-0.0584827  -0.57545169  0.07122067  0.79436168]\n",
      "[-0.06999173 -0.3813758   0.0871079   0.52490703]\n",
      "[-0.07761925 -0.57760864  0.09760604  0.84371761]\n",
      "[-0.08917142 -0.77391744  0.11448039  1.16542989]\n",
      "[-0.10464977 -0.58045634  0.13778899  0.91072153]\n",
      "[-0.1162589  -0.77714681  0.15600342  1.24334281]\n",
      "[-0.13180183 -0.5843323   0.18087028  1.00331252]\n",
      "[-0.14348848 -0.39202675  0.20093653  0.77244607]\n",
      "Episode finished after 39 timesteps\n",
      "[-0.00931283 -0.02008992 -0.01392491  0.04476035]\n",
      "[-0.00971463 -0.21500945 -0.0130297   0.33301752]\n",
      "[-0.01401482 -0.01970449 -0.00636935  0.03625432]\n",
      "[-0.01440891  0.17550821 -0.00564426 -0.25843137]\n",
      "[-0.01089874  0.37071029 -0.01081289 -0.55288923]\n",
      "[-0.00348454  0.17574184 -0.02187068 -0.26363258]\n",
      "[ 3.02991287e-05 -1.90612208e-02 -2.71433282e-02  2.20726614e-02]\n",
      "[-0.00035093  0.17643926 -0.02670187 -0.2790492 ]\n",
      "[ 0.00317786 -0.0182918  -0.03228286  0.00509384]\n",
      "[ 0.00281202 -0.21293625 -0.03218098  0.28741888]\n",
      "[-0.0014467  -0.0173705  -0.0264326  -0.01523743]\n",
      "[-0.00179411  0.17812036 -0.02673735 -0.31614157]\n",
      "[ 0.0017683  -0.01661075 -0.03306018 -0.03200916]\n",
      "[ 0.00143608 -0.21124338 -0.03370037  0.25006235]\n",
      "[-0.00278879 -0.40586828 -0.02869912  0.53192789]\n",
      "[-0.01090615 -0.60057506 -0.01806056  0.81543141]\n",
      "[-0.02291765 -0.40521052 -0.00175193  0.51712288]\n",
      "[-0.03102186 -0.21006395  0.00859052  0.2238884 ]\n",
      "[-0.03522314 -0.01506583  0.01306829 -0.06607241]\n",
      "[-0.03552446  0.17986635  0.01174684 -0.35460373]\n",
      "[-0.03192713 -0.01542064  0.00465477 -0.05823997]\n",
      "[-0.03223554 -0.21060902  0.00348997  0.23590793]\n",
      "[-0.03644773 -0.01553711  0.00820813 -0.05567211]\n",
      "[-0.03675847 -0.21077578  0.00709469  0.2395892 ]\n",
      "[-0.04097398 -0.40599836  0.01188647  0.53450152]\n",
      "[-0.04909395 -0.21104557  0.0225765   0.24558752]\n",
      "[-0.05331486 -0.40648257  0.02748825  0.54530525]\n",
      "[-0.06144451 -0.60197977  0.03839435  0.84652079]\n",
      "[-0.07348411 -0.79760388  0.05532477  1.15102587]\n",
      "[-0.08943619 -0.60324576  0.07834529  0.87619194]\n",
      "[-0.1015011  -0.7993402   0.09586913  1.19244129]\n",
      "[-0.11748791 -0.60558186  0.11971795  0.93128074]\n",
      "[-0.12959954 -0.41226119  0.13834357  0.67848838]\n",
      "[-0.13784477 -0.60900627  0.15191333  1.01132977]\n",
      "[-0.15002489 -0.41620169  0.17213993  0.76994453]\n",
      "[-0.15834893 -0.22381396  0.18753882  0.5359876 ]\n",
      "[-0.1628252  -0.42100906  0.19825857  0.88140763]\n",
      "Episode finished after 37 timesteps\n",
      "[ 0.0319198  -0.00372929 -0.01092376  0.00980843]\n",
      "[ 0.03184521 -0.19869289 -0.01072759  0.29902486]\n",
      "[ 0.02787135 -0.3936603  -0.0047471   0.58830529]\n",
      "[ 0.01999815 -0.58871545  0.00701901  0.87948909]\n",
      "[ 0.00822384 -0.39368957  0.02460879  0.58902104]\n",
      "[ 0.00035005 -0.1989207   0.03638921  0.30419045]\n",
      "[-0.00362837 -0.00433573  0.04247302  0.02320228]\n",
      "[-0.00371508 -0.20004023  0.04293707  0.32897761]\n",
      "[-0.00771589 -0.00555499  0.04951662  0.05013824]\n",
      "[-0.00782699  0.18882326  0.05051938 -0.22651982]\n",
      "[-0.00405052 -0.00698292  0.04598899  0.0816611 ]\n",
      "[-0.00419018  0.18745064  0.04762221 -0.19616488]\n",
      "[-0.00044117 -0.008319    0.04369891  0.1111523 ]\n",
      "[-0.00060755 -0.204039    0.04592196  0.41729543]\n",
      "[-0.00468833 -0.39978064  0.05426787  0.72409466]\n",
      "[-0.01268394 -0.59560944  0.06874976  1.03335235]\n",
      "[-0.02459613 -0.40146583  0.08941681  0.76302139]\n",
      "[-0.03262545 -0.20768177  0.10467724  0.49976008]\n",
      "[-0.03677908 -0.01417921  0.11467244  0.24181394]\n",
      "[-0.03706266  0.17913389  0.11950872 -0.01261142]\n",
      "[-0.03347999  0.37235723  0.11925649 -0.26532716]\n",
      "[-0.02603284  0.17575294  0.11394994  0.06246303]\n",
      "[-0.02251778  0.36907234  0.1151992  -0.19220665]\n",
      "[-0.01513634  0.17250702  0.11135507  0.13448076]\n",
      "[-0.0116862   0.36587238  0.11404469 -0.12110008]\n",
      "[-0.00436875  0.55919136  0.11162269 -0.37573833]\n",
      "[ 0.00681508  0.75256552  0.10410792 -0.63124545]\n",
      "[ 0.02186639  0.94609265  0.09148301 -0.88941195]\n",
      "[ 0.04078824  1.13986209  0.07369477 -1.15199218]\n",
      "[ 0.06358548  0.94386014  0.05065493 -0.8371406 ]\n",
      "[ 0.08246269  0.74808431  0.03391211 -0.52896735]\n",
      "[ 0.09742437  0.94271317  0.02333277 -0.81077453]\n",
      "[ 0.11627864  1.13750782  0.00711728 -1.09602777]\n",
      "[ 0.13902879  0.94229286 -0.01480328 -0.80112029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15787465  0.74737704 -0.03082568 -0.51313059]\n",
      "[ 0.17282219  0.94291929 -0.0410883  -0.81536593]\n",
      "[ 0.19168058  1.13857904 -0.05739561 -1.12068442]\n",
      "[ 0.21445216  1.33440489 -0.0798093  -1.43080454]\n",
      "[ 0.24114025  1.53041617 -0.10842539 -1.74732463]\n",
      "[ 0.27174858  1.33668071 -0.14337189 -1.49024258]\n",
      "[ 0.29848219  1.14356576 -0.17317674 -1.24554931]\n",
      "[ 0.32135351  0.95103553 -0.19808772 -1.01173353]\n",
      "Episode finished after 42 timesteps\n",
      "[ 0.02028419 -0.00322676  0.03235979  0.02456757]\n",
      "[ 0.02021965 -0.19879747  0.03285114  0.32728217]\n",
      "[ 0.0162437  -0.39437135  0.03939678  0.6301412 ]\n",
      "[ 0.00835627 -0.19982065  0.05199961  0.35012125]\n",
      "[ 0.00435986 -0.0054753   0.05900203  0.07427827]\n",
      "[ 0.00425035 -0.2013913   0.0604876   0.38497743]\n",
      "[ 2.22528506e-04 -3.97317491e-01  6.81871450e-02  6.96101751e-01]\n",
      "[-0.00772382 -0.59331553  0.08210918  1.0094471 ]\n",
      "[-0.01959013 -0.7894317   0.10229812  1.32674324]\n",
      "[-0.03537877 -0.59573906  0.12883299  1.06774641]\n",
      "[-0.04729355 -0.40253484  0.15018792  0.81811405]\n",
      "[-0.05534424 -0.59935834  0.1665502   1.15401527]\n",
      "[-0.06733141 -0.79621367  0.1896305   1.49395199]\n",
      "Episode finished after 13 timesteps\n",
      "[ 0.03143879 -0.01513548  0.00138601  0.02146005]\n",
      "[ 0.03113608 -0.21027729  0.00181521  0.31457995]\n",
      "[ 0.02693054 -0.01518124  0.00810681  0.02247003]\n",
      "[ 0.02662691  0.17982352  0.00855621 -0.26764412]\n",
      "[ 0.03022338 -0.01541949  0.00320333  0.02772517]\n",
      "[ 0.02991499 -0.21058723  0.00375783  0.32141706]\n",
      "[ 0.02570325 -0.015519    0.01018617  0.02992157]\n",
      "[ 0.02539287  0.17945541  0.0107846  -0.25953018]\n",
      "[ 0.02898198 -0.01581883  0.005594    0.03653475]\n",
      "[ 0.0286656   0.17922246  0.00632469 -0.25437798]\n",
      "[ 0.03225005 -0.01598922  0.00123713  0.04029314]\n",
      "[ 0.03193026 -0.21112889  0.002043    0.33336613]\n",
      "[ 0.02770769 -0.01603608  0.00871032  0.04132816]\n",
      "[ 0.02738696  0.17895989  0.00953688 -0.2485939 ]\n",
      "[ 0.03096616 -0.01629695  0.004565    0.04708185]\n",
      "[ 0.03064022 -0.21148406  0.00550664  0.34120157]\n",
      "[ 0.02641054 -0.01644089  0.01233067  0.05026019]\n",
      "[ 0.02608172 -0.21173746  0.01333588  0.3468079 ]\n",
      "[ 0.02184697 -0.40704654  0.02027203  0.6436661 ]\n",
      "[ 0.01370604 -0.60244507  0.03314536  0.94266325]\n",
      "[ 0.00165714 -0.40778504  0.05199862  0.66057653]\n",
      "[-0.00649856 -0.21342377  0.06521015  0.38470939]\n",
      "[-0.01076703 -0.01928529  0.07290434  0.11327881]\n",
      "[-0.01115274 -0.21537206  0.07516992  0.42804255]\n",
      "[-0.01546018 -0.02139072  0.08373077  0.15997138]\n",
      "[-0.01588799  0.1724389   0.08693019 -0.10516638]\n",
      "[-0.01243922 -0.02381431  0.08482687  0.21362714]\n",
      "[-0.0129155  -0.22004013  0.08909941  0.53181674]\n",
      "[-0.01731631 -0.02627695  0.09973574  0.26848545]\n",
      "[-0.01784184  0.16729069  0.10510545  0.00885061]\n",
      "[-0.01449603  0.36076055  0.10528247 -0.24890855]\n",
      "[-0.00728082  0.55423374  0.10030429 -0.50661553]\n",
      "[ 0.00380385  0.35785207  0.09017198 -0.18408358]\n",
      "[ 0.0109609   0.55157586  0.08649031 -0.44701403]\n",
      "[ 0.02199241  0.74537461  0.07755003 -0.71122909]\n",
      "[ 0.03689991  0.54926929  0.06332545 -0.39517833]\n",
      "[ 0.04788529  0.35330865  0.05542188 -0.0832213 ]\n",
      "[ 0.05495146  0.54759415  0.05375746 -0.3579164 ]\n",
      "[ 0.06590335  0.74191229  0.04659913 -0.63317546]\n",
      "[ 0.08074159  0.93635426  0.03393562 -0.91082662]\n",
      "[ 0.09946868  1.13100096  0.01571909 -1.19265341]\n",
      "[ 0.1220887   1.3259158  -0.00813398 -1.48036833]\n",
      "[ 0.14860701  1.13089405 -0.03774135 -1.19023675]\n",
      "[ 0.17122489  0.93628098 -0.06154608 -0.90961834]\n",
      "[ 0.18995051  1.13217949 -0.07973845 -1.22099287]\n",
      "[ 0.2125941   0.93817047 -0.10415831 -0.95432229]\n",
      "[ 0.23135751  1.13452774 -0.12324475 -1.27782913]\n",
      "[ 0.25404807  0.94117343 -0.14880133 -1.02613986]\n",
      "[ 0.27287154  1.13792932 -0.16932413 -1.36159866]\n",
      "[ 0.29563012  1.33471977 -0.19655611 -1.7021066 ]\n",
      "Episode finished after 50 timesteps\n",
      "[-0.02247842 -0.01394264  0.03606977 -0.00575647]\n",
      "[-0.02275728  0.18064395  0.03595464 -0.28684429]\n",
      "[-0.0191444   0.3752352   0.03021776 -0.56797412]\n",
      "[-0.01163969  0.56992055  0.01885828 -0.85098616]\n",
      "[-2.41281984e-04  7.64780361e-01  1.83855227e-03 -1.13767990e+00]\n",
      "[ 0.01505433  0.95987822 -0.02091505 -1.42978566]\n",
      "[ 0.03425189  1.15525207 -0.04951076 -1.72893092]\n",
      "[ 0.05735693  0.96072955 -0.08408938 -1.45205491]\n",
      "[ 0.07657152  0.76673525 -0.11313048 -1.18678481]\n",
      "[ 0.09190623  0.9631276  -0.13686617 -1.51267965]\n",
      "[ 0.11116878  1.15961576 -0.16711976 -1.84476883]\n",
      "[ 0.13436109  0.9666845  -0.20401514 -1.60830866]\n",
      "Episode finished after 12 timesteps\n",
      "[-0.00177251  0.00607852  0.02465147 -0.0154192 ]\n",
      "[-0.00165094 -0.18938813  0.02434308  0.28493849]\n",
      "[-0.0054387  -0.38484866  0.03004185  0.58519858]\n",
      "[-0.01313567 -0.19016011  0.04174583  0.3021285 ]\n",
      "[-0.01693888  0.00434277  0.0477884   0.02289808]\n",
      "[-0.01685202 -0.19143078  0.04824636  0.33026759]\n",
      "[-0.02068064 -0.38720513  0.05485171  0.63776633]\n",
      "[-0.02842474 -0.58304733  0.06760704  0.94720613]\n",
      "[-0.04008569 -0.77901134  0.08655116  1.26034196]\n",
      "[-0.05566591 -0.58509653  0.111758    0.99597283]\n",
      "[-0.06736784 -0.78152125  0.13167745  1.32156033]\n",
      "[-0.08299827 -0.97803829  0.15810866  1.65238543]\n",
      "[-0.10255903 -1.17461416  0.19115637  1.98986186]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.01799843  0.02937168 -0.04644919  0.00849143]\n",
      "[-0.01741099  0.22512794 -0.04627936 -0.2984774 ]\n",
      "[-0.01290843  0.42087798 -0.05224891 -0.60538936]\n",
      "[-0.00449087  0.22652411 -0.0643567  -0.32961058]\n",
      "[ 3.96076487e-05  4.22500355e-01 -7.09489116e-02 -6.41874204e-01]\n",
      "[ 0.00848961  0.61853583 -0.0837864  -0.95602911]\n",
      "[ 0.02086033  0.42463462 -0.10290698 -0.69080201]\n",
      "[ 0.02935302  0.62102255 -0.11672302 -1.01402677]\n",
      "[ 0.04177347  0.4276345  -0.13700355 -0.76015723]\n",
      "[ 0.05032616  0.62435131 -0.1522067  -1.09262065]\n",
      "[ 0.06281319  0.821115   -0.17405911 -1.42893018]\n",
      "[ 0.07923549  0.62851688 -0.20263771 -1.19531362]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.04359221 -0.00417912  0.02993867 -0.04612186]\n",
      "[ 0.04350863  0.19050103  0.02901623 -0.32921056]\n",
      "[ 0.04731865  0.38519815  0.02243202 -0.61260374]\n",
      "[ 0.05502261  0.18977001  0.01017995 -0.31294081]\n",
      "[ 0.05881801  0.38474547  0.00392113 -0.60239599]\n",
      "[ 0.06651292  0.18956889 -0.00812679 -0.30848057]\n",
      "[ 0.0703043  -0.00543633 -0.0142964  -0.01837165]\n",
      "[ 0.07019557 -0.20035037 -0.01466383  0.26976653]\n",
      "[ 0.06618856 -0.00502226 -0.0092685  -0.02750514]\n",
      "[ 0.06608812  0.19023137 -0.00981861 -0.32309791]\n",
      "[ 0.06989275  0.38549175 -0.01628056 -0.61886099]\n",
      "[ 0.07760258  0.58083728 -0.02865778 -0.91662672]\n",
      "[ 0.08921933  0.77633477 -0.04699032 -1.21817678]\n",
      "[ 0.10474602  0.58184913 -0.07135385 -0.94058041]\n",
      "[ 0.116383    0.7778565  -0.09016546 -1.25480356]\n",
      "[ 0.13194013  0.58399734 -0.11526153 -0.99166879]\n",
      "[ 0.14362008  0.39059065 -0.13509491 -0.73729561]\n",
      "[ 0.15143189  0.58729405 -0.14984082 -1.06925955]\n",
      "[ 0.16317777  0.39443698 -0.17122601 -0.82710395]\n",
      "[ 0.17106651  0.59143463 -0.18776809 -1.16837409]\n",
      "Episode finished after 20 timesteps\n",
      "[ 0.00923603 -0.02614274  0.01601466  0.01086328]\n",
      "[ 0.00871317 -0.22149066  0.01623192  0.30855569]\n",
      "[ 0.00428336 -0.41684008  0.02240304  0.6063132 ]\n",
      "[-0.00405344 -0.22203844  0.0345293   0.32077   ]\n",
      "[-0.00849421 -0.0274248   0.0409447   0.03917314]\n",
      "[-0.00904271  0.16708682  0.04172816 -0.24031536]\n",
      "[-0.00570097  0.3615886   0.03692186 -0.51954955]\n",
      "[ 0.0015308   0.16596684  0.02653087 -0.21546434]\n",
      "[ 0.00485014  0.36069966  0.02222158 -0.4996616 ]\n",
      "[ 0.01206413  0.5555014   0.01222835 -0.78525947]\n",
      "[ 0.02317416  0.36021358 -0.00347684 -0.4887546 ]\n",
      "[ 0.03037843  0.16514086 -0.01325193 -0.19716947]\n",
      "[ 0.03368125  0.36044983 -0.01719532 -0.49400316]\n",
      "[ 0.04089024  0.16557456 -0.02707539 -0.20678864]\n",
      "[ 0.04420174  0.361073   -0.03121116 -0.50788802]\n",
      "[ 0.0514232   0.16640439 -0.04136892 -0.22520191]\n",
      "[ 0.05475128  0.36209242 -0.04587296 -0.53064181]\n",
      "[ 0.06199313  0.16764476 -0.05648579 -0.25275906]\n",
      "[ 0.06534603  0.36352587 -0.06154098 -0.56271012]\n",
      "[ 0.07261654  0.16931903 -0.07279518 -0.29003295]\n",
      "[ 0.07600293  0.36539943 -0.07859584 -0.60475748]\n",
      "[ 0.08331091  0.56152744 -0.09069099 -0.9211247 ]\n",
      "[ 0.09454146  0.75775018 -0.10911348 -1.24087582]\n",
      "[ 0.10969647  0.95409063 -0.133931   -1.56565081]\n",
      "[ 0.12877828  1.15053483 -0.16524401 -1.89693637]\n",
      "[ 0.15178898  1.34701724 -0.20318274 -2.23600628]\n",
      "Episode finished after 26 timesteps\n",
      "[ 0.03753313 -0.02683599  0.01598695  0.03423913]\n",
      "[ 0.03699641  0.1680531   0.01667173 -0.25335718]\n",
      "[ 0.04035747 -0.02730288  0.01160459  0.04453734]\n",
      "[ 0.03981141  0.16765076  0.01249533 -0.24446176]\n",
      "[ 0.04316443 -0.02764742  0.0076061   0.05213616]\n",
      "[ 0.04261148  0.16736465  0.00864882 -0.2381373 ]\n",
      "[ 0.04595877 -0.02787979  0.00388608  0.05726112]\n",
      "[ 0.04540118  0.16718623  0.0050313  -0.2341932 ]\n",
      "[ 0.0487449  -0.02800724  0.00034743  0.06007251]\n",
      "[ 0.04818476  0.16710972  0.00154888 -0.23250078]\n",
      "[ 0.05152695 -0.02803432 -0.00310113  0.06067031]\n",
      "[ 0.05096626 -0.22311168 -0.00188773  0.3523732 ]\n",
      "[ 0.04650403 -0.41820673  0.00515974  0.64446027]\n",
      "[ 0.0381399  -0.22315707  0.01804894  0.35340662]\n",
      "[ 0.03367675 -0.41853095  0.02511708  0.65172582]\n",
      "[ 0.02530613 -0.22376765  0.03815159  0.36705666]\n",
      "[ 0.02083078 -0.02920801  0.04549273  0.08664342]\n",
      "[ 0.02024662  0.16523331  0.04722559 -0.19134656]\n",
      "[ 0.02355129  0.35964897  0.04339866 -0.46876575]\n",
      "[ 0.03074427  0.16394165  0.03402335 -0.16272598]\n",
      "[ 0.0340231   0.35856044  0.03076883 -0.44448427]\n",
      "[ 0.04119431  0.16301697  0.02187914 -0.14226329]\n",
      "[ 0.04445465  0.35781885  0.01903388 -0.42796422]\n",
      "[ 0.05161103  0.16243257  0.01047459 -0.12934227]\n",
      "[ 0.05485968  0.35740292  0.00788775 -0.41870227]\n",
      "[ 6.20077352e-02  5.52412212e-01 -4.86297760e-04 -7.08888143e-01]\n",
      "[ 0.07305598  0.357297   -0.01466406 -0.41635833]\n",
      "[ 0.08020192  0.55262367 -0.02299123 -0.71362792]\n",
      "[ 0.09125439  0.74805626 -0.03726379 -1.01345814]\n",
      "[ 0.10621552  0.94365493 -0.05753295 -1.3176055 ]\n",
      "[ 0.12508862  1.13945541 -0.08388506 -1.62772562]\n",
      "[ 0.14787772  1.3354575  -0.11643957 -1.94532825]\n",
      "[ 0.17458687  1.53161167 -0.15534614 -2.27172307]\n",
      "[ 0.20521911  1.72780318 -0.2007806  -2.60795483]\n",
      "Episode finished after 34 timesteps\n",
      "[ 0.00534787 -0.04672191 -0.04017179  0.04211811]\n",
      "[ 0.00441343 -0.24124547 -0.03932943  0.32186071]\n",
      "[-0.00041148 -0.04558617 -0.03289221  0.01703884]\n",
      "[-0.0013232  -0.24022134 -0.03255143  0.29916512]\n",
      "[-0.00612763 -0.04465089 -0.02656813 -0.00360359]\n",
      "[-0.00702065 -0.23938194 -0.0266402   0.28057979]\n",
      "[-0.01180829 -0.43411394 -0.02102861  0.56474286]\n",
      "[-0.02049056 -0.62893464 -0.00973375  0.85072734]\n",
      "[-0.03306926 -0.43368133  0.0072808   0.55499957]\n",
      "[-0.04174288 -0.23866236  0.01838079  0.26461941]\n",
      "[-0.04651613 -0.43404178  0.02367318  0.56304267]\n",
      "[-0.05519697 -0.23925987  0.03493403  0.27791094]\n",
      "[-0.05998216 -0.04465326  0.04049225 -0.00355236]\n",
      "[-0.06087523 -0.24033183  0.0404212   0.30162637]\n",
      "[-0.06568187 -0.0458086   0.04645373  0.02196046]\n",
      "[-0.06659804 -0.2415649   0.04689294  0.32893063]\n",
      "[-0.07142934 -0.43732195  0.05347155  0.6360249 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08017577 -0.24298493  0.06619205  0.36064914]\n",
      "[-0.08503547 -0.43898226  0.07340503  0.67344758]\n",
      "[-0.09381512 -0.24495318  0.08687398  0.40474942]\n",
      "[-0.09871418 -0.05116375  0.09496897  0.14067033]\n",
      "[-0.09973746  0.1424788   0.09778238 -0.1206057 ]\n",
      "[-0.09688788  0.33607371  0.09537026 -0.38090865]\n",
      "[-0.09016641  0.13973604  0.08775209 -0.0597441 ]\n",
      "[-0.08737169  0.33349736  0.08655721 -0.32350175]\n",
      "[-0.08070174  0.527287    0.08008717 -0.58768191]\n",
      "[-0.070156    0.72120143  0.06833353 -0.85410077]\n",
      "[-0.05573197  0.52521798  0.05125152 -0.54073725]\n",
      "[-0.04522761  0.3294145   0.04043677 -0.23235634]\n",
      "[-0.03863932  0.52393603  0.03578965 -0.51201495]\n",
      "[-0.0281606   0.71853607  0.02554935 -0.79320816]\n",
      "[-0.01378988  0.91329814  0.00968519 -1.07774524]\n",
      "[ 0.00447608  1.10829084 -0.01186972 -1.36737317]\n",
      "[ 0.0266419   1.30355934 -0.03921718 -1.6637449 ]\n",
      "[ 0.05271309  1.10891526 -0.07249208 -1.38353016]\n",
      "[ 0.07489139  1.30486274 -0.10016268 -1.69797344]\n",
      "[ 0.10098865  1.11102823 -0.13412215 -1.43807757]\n",
      "[ 0.12320921  0.91778984 -0.1628837  -1.19013695]\n",
      "[ 0.14156501  1.11460388 -0.18668644 -1.52912673]\n",
      "Episode finished after 39 timesteps\n",
      "[ 0.03259388 -0.04144922 -0.03534887 -0.02139242]\n",
      "[ 0.03176489  0.15416137 -0.03577672 -0.32501541]\n",
      "[ 0.03484812 -0.04043339 -0.04227703 -0.04382616]\n",
      "[ 0.03403945 -0.23492441 -0.04315355  0.23522395]\n",
      "[ 0.02934097 -0.03921332 -0.03844907 -0.07075259]\n",
      "[ 0.0285567  -0.23376356 -0.03986413  0.20955583]\n",
      "[ 0.02388143 -0.03809495 -0.03567301 -0.09543085]\n",
      "[ 0.02311953  0.15751965 -0.03758163 -0.3991517 ]\n",
      "[ 0.02626992  0.35315402 -0.04556466 -0.70344245]\n",
      "[ 0.033333    0.54887683 -0.05963351 -1.01011333]\n",
      "[ 0.04431054  0.74474176 -0.07983578 -1.32091034]\n",
      "[ 0.05920537  0.94077682 -0.10625398 -1.63747312]\n",
      "[ 0.07802091  0.74704927 -0.13900344 -1.37970055]\n",
      "[ 0.0929619   0.55390924 -0.16659746 -1.13352042]\n",
      "[ 0.10404008  0.7507725  -0.18926786 -1.47348026]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.02073653  0.04677505 -0.02779624  0.02644226]\n",
      "[-0.01980103 -0.14793748 -0.02726739  0.3102272 ]\n",
      "[-0.02275978  0.04756214 -0.02106285  0.00907116]\n",
      "[-0.02180854 -0.14725151 -0.02088142  0.29503479]\n",
      "[-0.02475357  0.04816182 -0.01498073 -0.00415999]\n",
      "[-0.02379033  0.24349538 -0.01506393 -0.30153162]\n",
      "[-0.01892042  0.04859133 -0.02109456 -0.01363735]\n",
      "[-0.0179486   0.24400935 -0.02136731 -0.3129005 ]\n",
      "[-0.01306841  0.43942908 -0.02762532 -0.61224469]\n",
      "[-0.00427983  0.634926   -0.03987021 -0.913499  ]\n",
      "[ 0.00841869  0.83056395 -0.05814019 -1.21844144]\n",
      "[ 0.02502997  0.63623779 -0.08250902 -0.9445283 ]\n",
      "[ 0.03775473  0.83236847 -0.10139959 -1.26195362]\n",
      "[ 0.0544021   0.63867873 -0.12663866 -1.00267178]\n",
      "[ 0.06717567  0.8352442  -0.14669209 -1.33229118]\n",
      "[ 0.08388055  0.64224457 -0.17333792 -1.08887393]\n",
      "[ 0.09672545  0.44977846 -0.1951154  -0.85520757]\n",
      "Episode finished after 17 timesteps\n",
      "[ 0.00778094 -0.03563226 -0.02576629 -0.03503656]\n",
      "[ 0.00706829 -0.23037542 -0.02646702  0.24940677]\n",
      "[ 0.00246078 -0.0348857  -0.02147889 -0.05150551]\n",
      "[ 0.00176307  0.16053755 -0.022509   -0.35088707]\n",
      "[ 0.00497382 -0.03425718 -0.02952674 -0.06538608]\n",
      "[ 0.00428867 -0.22894363 -0.03083446  0.2178367 ]\n",
      "[-2.90197714e-04 -4.23611542e-01 -2.64777270e-02  5.00635870e-01]\n",
      "[-0.00876243 -0.61835043 -0.01646501  0.78485827]\n",
      "[-0.02112944 -0.42300616 -0.00076784  0.48704108]\n",
      "[-0.02958956 -0.61811726  0.00897298  0.77948191]\n",
      "[-0.04195191 -0.42311982  0.02456262  0.48963554]\n",
      "[-0.0504143  -0.61857952  0.03435533  0.78995734]\n",
      "[-0.06278589 -0.814156    0.05015447  1.09324742]\n",
      "[-0.07906901 -0.61972943  0.07201942  0.81671353]\n",
      "[-0.0914636  -0.81575958  0.08835369  1.13115192]\n",
      "[-0.10777879 -0.62189846  0.11097673  0.86743672]\n",
      "[-0.12021676 -0.81834135  0.12832547  1.19284862]\n",
      "[-0.13658359 -1.01487038  0.15218244  1.52284247]\n",
      "[-0.156881   -0.82187894  0.18263929  1.28127019]\n",
      "[-0.17331858 -0.62949193  0.20826469  1.05088733]\n",
      "Episode finished after 20 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Two-Armed Bandit\n",
    "\n",
    "The simplest reinforcement learning problem is the n-armed bandit. Essentially, there are n-many slot machines, each with a different fixed payout probability. The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. We are going to make it even simpler, by only having two possible slot machines to choose between. In fact, this problem is so simple that it is more of a precursor to real RL problems than one itself. \n",
    "\n",
    "Actually, typical aspects of a task that make it an RL problem are the following:\n",
    "\n",
    "- Different actions yield different rewards. For example, when looking for treasure in a maze, going left may lead to the treasure, whereas going right may lead to a pit of snakes.\n",
    "- Rewards are delayed over time. This just means that even if going left in the above example is the right thing to do, we may not know it till later in the maze.\n",
    "- Reward for an action is conditional on the state of the environment. Continuing the maze example, going left may be ideal at a certain fork in the path, but not at others.\n",
    "\n",
    "The n-armed bandit is a nice starting place because we don’t have to worry about aspects #2 and 3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones. \n",
    "\n",
    "In the context of RL lingo, this is called learning a policy. We are going to be using a method called policy gradients, where our simple neural network learns a policy for picking actions by adjusting its weights through gradient descent using feedback from the environment. \n",
    "There is another approach to reinforcement learning where agents learn value functions. In those approaches, instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in. Both approaches allow agents to learn good behavior, but the policy gradient approach is a little more direct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Q-Learning and Exploration\n",
    "\n",
    "Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we are going to be attempting to solve the [FrozenLake](https://gym.openai.com/envs/FrozenLake-v0) environment from the OpenAI gym. For those unfamiliar, the [OpenAI gym](https://gym.openai.com/) provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.\n",
    "\n",
    "In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n",
    "\n",
    "We make updates to our Q-table using something called the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation), which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:\n",
    "                \n",
    "Eq 1. $$ Q(s,a) = r + \\gamma ( \\max_{a'}(Q(s',a') ) $$ \n",
    "                \n",
    "                \n",
    "This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (γ) future reward expected according to our own table for the next state (s’) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-08 08:10:30,184] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.47\n",
      "Final Q-Table Values\n",
      "[[1.56766843e-01 3.18627129e-03 3.67528453e-03 3.77893913e-03]\n",
      " [2.24844370e-03 1.81892896e-03 1.44296408e-03 1.88495771e-01]\n",
      " [6.63991590e-02 3.13524897e-03 9.80783117e-04 3.53695420e-03]\n",
      " [2.83684893e-04 6.89582769e-04 3.54765191e-04 3.32624378e-03]\n",
      " [2.30442892e-01 2.49814634e-03 4.91045424e-04 3.00214840e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.31496326e-02 1.49498645e-04 3.61531385e-04 2.23060493e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.53914355e-04 7.30102203e-04 7.14387222e-04 4.46942197e-01]\n",
      " [0.00000000e+00 7.08253345e-01 2.13962908e-03 0.00000000e+00]\n",
      " [1.37664993e-01 2.19365718e-04 8.45954175e-04 2.58101457e-05]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 7.58326452e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.57854007e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load the environement\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Implement Q-Table Learning Algorithm\n",
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)\n",
    "\n",
    "print \"Score over time: \" +  str(sum(rList)/num_episodes)\n",
    "print \"Final Q-Table Values\"\n",
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Q-Learning with Neural Networks\n",
    "\n",
    "Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.\n",
    "\n",
    "\n",
    "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the “target” value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.\n",
    "\n",
    "            Eq2. Loss = ∑(Q-target - Q)²\n",
    "            \n",
    "Below is the Tensorflow walkthrough of implementing our simple Q-Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-08 08:10:45,046] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the environement\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# The Q-Network Approach\n",
    "# Implementing the network itself\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of succesful episodes: 0.1435%\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "e = 0.1\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            #Get new state and reward from environment\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistics on network performance\n",
    "We can see that the network beings to consistly reach the goal around the 750 episode mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc914079950>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also begins to progress through the environment for longer than chance aroudn the 750 mark as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc90cfb7d90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the network learns to solve the FrozenLake problem, it turns out it doesn’t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Policy Gradient\n",
    "\n",
    "The simplest way to think of a Policy gradient network is one which produces explicit outputs. In the case of our bandit, we don’t need to condition these outputs on any state. As such, our network will consist of just a set of weights, with each corresponding to each of the possible arms to pull in the bandit, and will represent how good our agent thinks it is to pull each arm. If we initialize these weights to 1, then our agent will be somewhat optimistic about each arm’s potential reward.\n",
    "\n",
    "To update our network, we will simply try an arm with an e-greedy policy (See Part 7 for more on action-selection strategies). This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly. In this way, the agent can try out each of the different arms to continue to learn more about them. Once our agent has taken an action, it then receives a reward of either 1 or -1. With this reward, we can then make an update to our network using the policy loss equation:\n",
    "\n",
    "                Loss = -log(π)*A\n",
    "\n",
    "A is advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline. In future algorithms, we will develop more complex baselines to compare our rewards to, but for now we will assume that the baseline is 0, and it can be thought of as simply the reward we received for each action.\n",
    "\n",
    "π is the policy. In this case, it corresponds to the chosen action’s weight.\n",
    "\n",
    "Intuitively, this loss function allows us to increase the weight for actions that yielded a positive reward, and decrease them for actions that yielded a negative reward. In this way the agent will be more or less likely to pick that action in the future. By taking actions, getting rewards, and updating our network in this circular manner, we will quickly converge to an agent that can solve our bandit problem! Don’t take my word for it though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial: The Multi-armed bandit\n",
    "\n",
    "This tutorial contains a simple example of how to build a policy-gradient based agent that can solve the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bandits\n",
    "\n",
    "Here we define our bandits. For this example we are using a four-armed bandit. The pullBandit function generates a random number from a normal distribution with a mean of 0. The lower the bandit number, the more likely a positive reward will be returned. We want our agent to learn to always choose the bandit that will give that positive reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#List out our bandits. Currently bandit 4 (index#3) is set to most often provide a positive reward.\n",
    "bandits = [0.2,0,-0.2,-5]\n",
    "num_bandits = len(bandits)\n",
    "def pullBandit(bandit):\n",
    "    #Get a random number.\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        #return a negative reward.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "The code below established our simple neural agent. It consists of a set of values for each of the bandits. Each value is an estimate of the value of the return from choosing the bandit. We use a policy gradient method to update the agent by moving the value for the selected action toward the recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#These two lines established the feed-forward part of the network. This does the actual choosing.\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights,0)\n",
    "\n",
    "#The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "#to compute the loss, and use it to update the network.\n",
    "reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights,action_holder,[1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "We will train our agent by taking actions in our environment, and recieving rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 bandits: [-1.  0.  0.  0.]\n",
      "Running reward for the 4 bandits: [ 0. -1.  0. 46.]\n",
      "Running reward for the 4 bandits: [ 0.  2.  0. 93.]\n",
      "Running reward for the 4 bandits: [ -1.   2.   1. 139.]\n",
      "Running reward for the 4 bandits: [ -2.   3.  -1. 183.]\n",
      "Running reward for the 4 bandits: [ -3.   4.   1. 227.]\n",
      "Running reward for the 4 bandits: [ -2.   4.   2. 275.]\n",
      "Running reward for the 4 bandits: [ -1.   4.   2. 320.]\n",
      "Running reward for the 4 bandits: [ -1.   4.   2. 370.]\n",
      "Running reward for the 4 bandits: [ -2.   4.   5. 414.]\n",
      "Running reward for the 4 bandits: [ -1.   4.   6. 462.]\n",
      "Running reward for the 4 bandits: [ -2.   4.   6. 509.]\n",
      "Running reward for the 4 bandits: [ -3.   5.   7. 554.]\n",
      "Running reward for the 4 bandits: [ -3.   6.   7. 603.]\n",
      "Running reward for the 4 bandits: [ -3.   7.   7. 650.]\n",
      "Running reward for the 4 bandits: [ -4.   7.   7. 697.]\n",
      "Running reward for the 4 bandits: [ -4.   6.   5. 744.]\n",
      "Running reward for the 4 bandits: [ -4.   7.   6. 790.]\n",
      "Running reward for the 4 bandits: [ -4.   6.   7. 836.]\n",
      "Running reward for the 4 bandits: [ -5.   5.   8. 881.]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #Set total number of episodes to train agent on.\n",
    "total_reward = np.zeros(num_bandits) #Set scoreboard for bandits to 0.\n",
    "e = 0.1 #Set the chance of taking a random action.\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #Choose either a random action or one from our network.\n",
    "        if np.random.rand(1) < e:\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        \n",
    "        reward = pullBandit(bandits[action]) #Get our reward from picking one of the bandits.\n",
    "        \n",
    "        #Update the network.\n",
    "        _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]})\n",
    "        \n",
    "        #Update our running tally of scores.\n",
    "        total_reward[action] += reward\n",
    "        if i % 50 == 0:\n",
    "            print \"Running reward for the \" + str(num_bandits) + \" bandits: \" + str(total_reward)\n",
    "        i+=1\n",
    "print \"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" is the most promising....\"\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits)):\n",
    "    print \"...and it was right!\"\n",
    "else:\n",
    "    print \"...and it was wrong!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7. Policy-based Agents\n",
    "\n",
    "Environments which pose the full problem to an agent are referred to as Markov Decision Processes (MDPs). These environments not only provide rewards and state transitions given actions, but those rewards are also condition on the state of the environment and the action the agent takes within that state. These dynamics are also temporal, and can be delayed over time.\n",
    "\n",
    "To be a little more formal, we can define a Markov Decision Process as follows. An MDP consists of a set of all possible states S from which our agent at any time will experience s. A set of all possible actions A from which our agent at any time will take action a. Given a state action pair (s, a), the transition probability to a new state s’ is defined by T(s, a), and the reward r is given by R(s, a). As such, at any time in an MDP, an agent is given a state s, takes action a, and receives new state s’ and reward r.\n",
    "\n",
    "While it may seem relatively simple, we can pose almost any task we could think of as an MDP. For example, imagine opening a door. The state is the vision of the door that we have, as well as the position of our body and door in the world. The actions are our every movement our body could make. The reward in this case is the door successfully opening. Certain actions, like walking toward the door are essential to solving the problem, but aren’t themselves reward-giving, since only actually opening the door will provide the reward. In this way, an agent needs to learn to assign value to actions the lead eventually to the reward, hence the introduction of temporal dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Cart-Pole Task\n",
    "\n",
    "In order to accomplish this, we are going to need a challenge that is more difficult for the agent than the two-armed bandit. To meet provide this challenge we are going to utilize the OpenAI gym, a collection of reinforcement learning environments. We will be using one of the classic tasks, the Cart-Pole. To learn more about the OpenAI gym, and this specific task, check out their tutorial [here](https://gym.openai.com/docs). Essentially, we are going to have our agent learn how to balance a pole for as long as possible without it falling. Unlike the two-armed bandit, this task requires:\n",
    "\n",
    "- Observations — The agent needs to know where pole currently is, and the angle at which it is balancing. To accomplish this, our neural network will take an observation and use it when producing the probability of an action.\n",
    "\n",
    "- Delayed reward — Keeping the pole in the air as long as possible means moving in ways that will be advantageous for both the present and the future. To accomplish this we will adjust the reward value for each observation-action pair using a function that weighs actions over time.\n",
    "\n",
    "To take reward over time into account, the form of Policy Gradient we used in the previous tutorials will need a few adjustments. The first of which is that we now need to update our agent with more than one experience at a time. To accomplish this, we will collect experiences in a buffer, and then occasionally use them to update the agent all at once. These sequences of experience are sometimes referred to as rollouts, or experience traces. We can’t just apply these rollouts by themselves however, we will need to ensure that the rewards are properly adjusted by a discount factor\n",
    "\n",
    "Intuitively this allows each action to be a little bit responsible for not only the immediate reward, but all the rewards that followed. We now use this modified reward as an estimation of the advantage in our loss equation. With those changes, we are ready to solve CartPole!\n",
    "\n",
    "And with that we have a fully-functional reinforcement learning agent. Our agent is still far from the state of the art though. While we are using a neural network for the policy, the network still isn’t as deep or complex as the most advanced networks. In the next post I will be showing how to use Deep Neural Networks to create agents able to learn more complex relationships with the environment in order to play a more exciting game than pole balancing. In doing so, I will be diving into the kinds of representations that a network learns for more complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8. Model-Based RL\n",
    "\n",
    "Now we will introduce the concept of a model of the environment that the agent can use to improve it’s performance.\n",
    "\n",
    "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world.\n",
    "\n",
    "Unlike in computer simulations, physical environments take time to navigate, and the physical rules of the world prevent things like easy environment resets from being feasible. Instead, we can save time and energy by building a model of the environment. With such a model, an agent can ‘imagine’ what it might be like to move around the real environment, and we can train a policy on this imagined environment in addition to the real one. If we were given a good enough model of an environment, an agent could be trained entirely on that model, and even perform well when placed into a real environment for the first time.\n",
    "\n",
    "How are we going to accomplish this in Tensorflow? As I mentioned above, we are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agent’s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! Read the iPython notebook below for the details on how this is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this following part we implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries and starting CartPole environment\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# from modelAny import *\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "# from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-06-08 08:58:46,146] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# model initialization\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None,4] , name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32,name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "loglik = tf.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5])\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None,5] , name=\"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]),name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state,W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]),name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M,W2M) + B2M)\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]),name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]),name=\"bR\")\n",
    "bD = tf.Variable(tf.ones([1]),name=\"bD\")\n",
    "\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M,wO,name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M,wR,name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M,wD,name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32,[None,4],name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32,[None,1],name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32,[None,1],name=\"true_done\")\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation,predicted_reward,predicted_done],1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1-predicted_done, 1-true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "        \n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]),[1,5])\n",
    "    myPredict = sess.run([predicted_state],feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:,0:4]\n",
    "    observation[:,0] = np.clip(observation[:,0],-2.4,2.4)\n",
    "    observation[:,2] = np.clip(observation[:,2],-0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:,5],0,1)\n",
    "    if doneP > 0.1 or len(xs)>= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4.000000. Reward 16.000000. action: 1.000000. mean reward 16.000000.\n",
      "World Perf: Episode 7.000000. Reward 21.000000. action: 1.000000. mean reward 16.050000.\n",
      "World Perf: Episode 10.000000. Reward 19.333333. action: 0.000000. mean reward 16.082833.\n",
      "World Perf: Episode 13.000000. Reward 20.333333. action: 1.000000. mean reward 16.125338.\n",
      "World Perf: Episode 16.000000. Reward 18.333333. action: 1.000000. mean reward 16.147418.\n",
      "World Perf: Episode 19.000000. Reward 18.333333. action: 0.000000. mean reward 16.169277.\n",
      "World Perf: Episode 22.000000. Reward 17.666667. action: 1.000000. mean reward 16.184251.\n",
      "World Perf: Episode 25.000000. Reward 35.000000. action: 0.000000. mean reward 16.372409.\n",
      "World Perf: Episode 28.000000. Reward 23.666667. action: 0.000000. mean reward 16.445351.\n",
      "World Perf: Episode 31.000000. Reward 21.000000. action: 1.000000. mean reward 16.490898.\n",
      "World Perf: Episode 34.000000. Reward 20.666667. action: 0.000000. mean reward 16.532656.\n",
      "World Perf: Episode 37.000000. Reward 16.666667. action: 1.000000. mean reward 16.533996.\n",
      "World Perf: Episode 40.000000. Reward 25.000000. action: 1.000000. mean reward 16.618656.\n",
      "World Perf: Episode 43.000000. Reward 31.333333. action: 1.000000. mean reward 16.765802.\n",
      "World Perf: Episode 46.000000. Reward 23.333333. action: 1.000000. mean reward 16.831478.\n",
      "World Perf: Episode 49.000000. Reward 14.333333. action: 1.000000. mean reward 16.806496.\n",
      "World Perf: Episode 52.000000. Reward 22.666667. action: 0.000000. mean reward 16.865098.\n",
      "World Perf: Episode 55.000000. Reward 24.666667. action: 0.000000. mean reward 16.943114.\n",
      "World Perf: Episode 58.000000. Reward 19.666667. action: 1.000000. mean reward 16.970349.\n",
      "World Perf: Episode 61.000000. Reward 16.000000. action: 1.000000. mean reward 16.960646.\n",
      "World Perf: Episode 64.000000. Reward 11.666667. action: 1.000000. mean reward 16.907706.\n",
      "World Perf: Episode 67.000000. Reward 18.666667. action: 1.000000. mean reward 16.925296.\n",
      "World Perf: Episode 70.000000. Reward 31.666667. action: 1.000000. mean reward 17.072709.\n",
      "World Perf: Episode 73.000000. Reward 22.000000. action: 1.000000. mean reward 17.121982.\n",
      "World Perf: Episode 76.000000. Reward 16.000000. action: 0.000000. mean reward 17.110762.\n",
      "World Perf: Episode 79.000000. Reward 14.000000. action: 0.000000. mean reward 17.079655.\n",
      "World Perf: Episode 82.000000. Reward 20.333333. action: 0.000000. mean reward 17.112192.\n",
      "World Perf: Episode 85.000000. Reward 41.333333. action: 0.000000. mean reward 17.354403.\n",
      "World Perf: Episode 88.000000. Reward 21.333333. action: 1.000000. mean reward 17.394192.\n",
      "World Perf: Episode 91.000000. Reward 19.333333. action: 0.000000. mean reward 17.413584.\n",
      "World Perf: Episode 94.000000. Reward 19.666667. action: 1.000000. mean reward 17.436115.\n",
      "World Perf: Episode 97.000000. Reward 29.333333. action: 0.000000. mean reward 17.555087.\n",
      "World Perf: Episode 100.000000. Reward 16.666667. action: 1.000000. mean reward 17.546203.\n",
      "World Perf: Episode 103.000000. Reward 16.333333. action: 1.000000. mean reward 17.534074.\n",
      "World Perf: Episode 106.000000. Reward 18.000000. action: 1.000000. mean reward 17.477552.\n",
      "World Perf: Episode 109.000000. Reward 23.000000. action: 1.000000. mean reward 17.461153.\n",
      "World Perf: Episode 112.000000. Reward 14.666667. action: 0.000000. mean reward 17.358727.\n",
      "World Perf: Episode 115.000000. Reward 20.333333. action: 0.000000. mean reward 20.345940.\n",
      "World Perf: Episode 118.000000. Reward 22.000000. action: 1.000000. mean reward 20.380720.\n",
      "World Perf: Episode 121.000000. Reward 18.333333. action: 0.000000. mean reward 20.235426.\n",
      "World Perf: Episode 124.000000. Reward 18.666667. action: 1.000000. mean reward 20.178297.\n",
      "World Perf: Episode 127.000000. Reward 29.666667. action: 1.000000. mean reward 20.180244.\n",
      "World Perf: Episode 130.000000. Reward 16.333333. action: 1.000000. mean reward 22.104982.\n",
      "World Perf: Episode 133.000000. Reward 22.666667. action: 0.000000. mean reward 22.136414.\n",
      "World Perf: Episode 136.000000. Reward 31.000000. action: 0.000000. mean reward 22.099960.\n",
      "World Perf: Episode 139.000000. Reward 23.666667. action: 1.000000. mean reward 23.134933.\n",
      "World Perf: Episode 142.000000. Reward 44.000000. action: 1.000000. mean reward 24.282974.\n",
      "World Perf: Episode 145.000000. Reward 23.000000. action: 1.000000. mean reward 24.128687.\n",
      "World Perf: Episode 148.000000. Reward 20.000000. action: 1.000000. mean reward 24.003004.\n",
      "World Perf: Episode 151.000000. Reward 26.000000. action: 1.000000. mean reward 23.928347.\n",
      "World Perf: Episode 154.000000. Reward 30.000000. action: 0.000000. mean reward 23.860817.\n",
      "World Perf: Episode 157.000000. Reward 27.666667. action: 1.000000. mean reward 24.008406.\n",
      "World Perf: Episode 160.000000. Reward 19.333333. action: 1.000000. mean reward 24.655434.\n",
      "World Perf: Episode 163.000000. Reward 27.000000. action: 1.000000. mean reward 24.560928.\n",
      "World Perf: Episode 166.000000. Reward 27.666667. action: 0.000000. mean reward 24.498934.\n",
      "World Perf: Episode 169.000000. Reward 21.000000. action: 0.000000. mean reward 24.309938.\n",
      "World Perf: Episode 172.000000. Reward 37.666667. action: 0.000000. mean reward 24.377226.\n",
      "World Perf: Episode 175.000000. Reward 19.666667. action: 1.000000. mean reward 24.204359.\n",
      "World Perf: Episode 178.000000. Reward 19.666667. action: 0.000000. mean reward 23.999350.\n",
      "World Perf: Episode 181.000000. Reward 20.333333. action: 0.000000. mean reward 23.794891.\n",
      "World Perf: Episode 184.000000. Reward 54.000000. action: 1.000000. mean reward 23.913546.\n",
      "World Perf: Episode 187.000000. Reward 26.666667. action: 0.000000. mean reward 25.934448.\n",
      "World Perf: Episode 190.000000. Reward 26.333333. action: 1.000000. mean reward 25.768316.\n",
      "World Perf: Episode 193.000000. Reward 27.000000. action: 1.000000. mean reward 25.637815.\n",
      "World Perf: Episode 196.000000. Reward 47.333333. action: 0.000000. mean reward 25.710730.\n",
      "World Perf: Episode 199.000000. Reward 58.333333. action: 0.000000. mean reward 25.822035.\n",
      "World Perf: Episode 202.000000. Reward 21.000000. action: 0.000000. mean reward 25.814285.\n",
      "World Perf: Episode 205.000000. Reward 55.666667. action: 0.000000. mean reward 25.987350.\n",
      "World Perf: Episode 208.000000. Reward 34.000000. action: 1.000000. mean reward 26.305780.\n",
      "World Perf: Episode 211.000000. Reward 29.000000. action: 0.000000. mean reward 26.156916.\n",
      "World Perf: Episode 214.000000. Reward 25.000000. action: 0.000000. mean reward 26.119362.\n",
      "World Perf: Episode 217.000000. Reward 27.333333. action: 1.000000. mean reward 25.958803.\n",
      "World Perf: Episode 220.000000. Reward 35.666667. action: 1.000000. mean reward 25.908995.\n",
      "World Perf: Episode 223.000000. Reward 17.666667. action: 1.000000. mean reward 25.677839.\n",
      "World Perf: Episode 226.000000. Reward 27.000000. action: 0.000000. mean reward 25.697479.\n",
      "World Perf: Episode 229.000000. Reward 27.666667. action: 0.000000. mean reward 25.625908.\n",
      "World Perf: Episode 232.000000. Reward 31.666667. action: 1.000000. mean reward 25.855688.\n",
      "World Perf: Episode 235.000000. Reward 16.333333. action: 0.000000. mean reward 25.722025.\n",
      "World Perf: Episode 238.000000. Reward 34.333333. action: 0.000000. mean reward 25.961649.\n",
      "World Perf: Episode 241.000000. Reward 35.666667. action: 0.000000. mean reward 25.881094.\n",
      "World Perf: Episode 244.000000. Reward 18.666667. action: 1.000000. mean reward 28.537127.\n",
      "World Perf: Episode 247.000000. Reward 14.666667. action: 0.000000. mean reward 31.071503.\n",
      "World Perf: Episode 250.000000. Reward 39.666667. action: 0.000000. mean reward 30.967695.\n",
      "World Perf: Episode 253.000000. Reward 30.333333. action: 0.000000. mean reward 31.018103.\n",
      "World Perf: Episode 256.000000. Reward 51.000000. action: 1.000000. mean reward 31.001890.\n",
      "World Perf: Episode 259.000000. Reward 30.666667. action: 0.000000. mean reward 30.842207.\n",
      "World Perf: Episode 262.000000. Reward 12.666667. action: 0.000000. mean reward 30.581137.\n",
      "World Perf: Episode 265.000000. Reward 32.333333. action: 0.000000. mean reward 30.412497.\n",
      "World Perf: Episode 268.000000. Reward 45.333333. action: 0.000000. mean reward 30.474508.\n",
      "World Perf: Episode 271.000000. Reward 37.000000. action: 0.000000. mean reward 30.611868.\n",
      "World Perf: Episode 274.000000. Reward 23.333333. action: 0.000000. mean reward 30.335047.\n",
      "World Perf: Episode 277.000000. Reward 39.666667. action: 1.000000. mean reward 30.190409.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 280.000000. Reward 56.000000. action: 0.000000. mean reward 30.339041.\n",
      "World Perf: Episode 283.000000. Reward 35.000000. action: 0.000000. mean reward 30.210157.\n",
      "World Perf: Episode 286.000000. Reward 25.333333. action: 0.000000. mean reward 29.932100.\n",
      "World Perf: Episode 289.000000. Reward 39.666667. action: 0.000000. mean reward 29.912317.\n",
      "World Perf: Episode 292.000000. Reward 24.666667. action: 1.000000. mean reward 29.723516.\n",
      "World Perf: Episode 295.000000. Reward 48.666667. action: 1.000000. mean reward 29.920038.\n",
      "World Perf: Episode 298.000000. Reward 35.333333. action: 1.000000. mean reward 29.784628.\n",
      "World Perf: Episode 301.000000. Reward 67.333333. action: 0.000000. mean reward 29.894615.\n",
      "World Perf: Episode 304.000000. Reward 23.666667. action: 0.000000. mean reward 29.709541.\n",
      "World Perf: Episode 307.000000. Reward 34.333333. action: 1.000000. mean reward 29.646063.\n",
      "World Perf: Episode 310.000000. Reward 18.666667. action: 1.000000. mean reward 31.979164.\n",
      "World Perf: Episode 313.000000. Reward 24.333333. action: 1.000000. mean reward 31.734247.\n",
      "World Perf: Episode 316.000000. Reward 37.000000. action: 0.000000. mean reward 31.637297.\n",
      "World Perf: Episode 319.000000. Reward 26.333333. action: 1.000000. mean reward 31.394583.\n",
      "World Perf: Episode 322.000000. Reward 30.666667. action: 0.000000. mean reward 31.247274.\n",
      "World Perf: Episode 325.000000. Reward 20.000000. action: 1.000000. mean reward 33.792152.\n",
      "World Perf: Episode 328.000000. Reward 25.333333. action: 1.000000. mean reward 33.444836.\n",
      "World Perf: Episode 331.000000. Reward 50.333333. action: 0.000000. mean reward 33.701328.\n",
      "World Perf: Episode 334.000000. Reward 34.000000. action: 0.000000. mean reward 33.540936.\n",
      "World Perf: Episode 337.000000. Reward 19.000000. action: 0.000000. mean reward 33.429424.\n",
      "World Perf: Episode 340.000000. Reward 36.000000. action: 0.000000. mean reward 33.279545.\n",
      "World Perf: Episode 343.000000. Reward 26.666667. action: 0.000000. mean reward 33.055485.\n",
      "World Perf: Episode 346.000000. Reward 24.333333. action: 0.000000. mean reward 35.354565.\n",
      "World Perf: Episode 349.000000. Reward 23.333333. action: 0.000000. mean reward 34.981838.\n",
      "World Perf: Episode 352.000000. Reward 26.666667. action: 1.000000. mean reward 37.755009.\n",
      "World Perf: Episode 355.000000. Reward 32.000000. action: 0.000000. mean reward 40.606335.\n",
      "World Perf: Episode 358.000000. Reward 33.666667. action: 1.000000. mean reward 40.325531.\n",
      "World Perf: Episode 361.000000. Reward 28.666667. action: 1.000000. mean reward 39.937496.\n",
      "World Perf: Episode 364.000000. Reward 71.666667. action: 1.000000. mean reward 39.981365.\n",
      "World Perf: Episode 367.000000. Reward 41.000000. action: 0.000000. mean reward 39.704842.\n",
      "World Perf: Episode 370.000000. Reward 28.333333. action: 1.000000. mean reward 41.970150.\n",
      "World Perf: Episode 373.000000. Reward 52.333333. action: 0.000000. mean reward 41.765911.\n",
      "World Perf: Episode 376.000000. Reward 70.333333. action: 1.000000. mean reward 45.211185.\n",
      "World Perf: Episode 379.000000. Reward 34.333333. action: 0.000000. mean reward 44.709366.\n",
      "World Perf: Episode 382.000000. Reward 23.333333. action: 1.000000. mean reward 44.119411.\n",
      "World Perf: Episode 385.000000. Reward 51.000000. action: 1.000000. mean reward 43.803146.\n",
      "World Perf: Episode 388.000000. Reward 33.333333. action: 0.000000. mean reward 43.306839.\n",
      "World Perf: Episode 391.000000. Reward 50.333333. action: 1.000000. mean reward 43.051788.\n",
      "World Perf: Episode 394.000000. Reward 27.333333. action: 0.000000. mean reward 42.570244.\n",
      "World Perf: Episode 397.000000. Reward 35.666667. action: 0.000000. mean reward 42.142895.\n",
      "World Perf: Episode 400.000000. Reward 54.000000. action: 1.000000. mean reward 41.900673.\n",
      "World Perf: Episode 403.000000. Reward 38.666667. action: 1.000000. mean reward 42.959259.\n",
      "World Perf: Episode 406.000000. Reward 34.666667. action: 1.000000. mean reward 44.267361.\n",
      "World Perf: Episode 409.000000. Reward 64.333333. action: 0.000000. mean reward 44.123276.\n",
      "World Perf: Episode 412.000000. Reward 58.000000. action: 0.000000. mean reward 44.006680.\n",
      "World Perf: Episode 415.000000. Reward 52.666667. action: 1.000000. mean reward 43.778702.\n",
      "World Perf: Episode 418.000000. Reward 94.333333. action: 0.000000. mean reward 43.948292.\n",
      "World Perf: Episode 421.000000. Reward 42.333333. action: 1.000000. mean reward 46.452221.\n",
      "World Perf: Episode 424.000000. Reward 36.666667. action: 0.000000. mean reward 45.960155.\n",
      "World Perf: Episode 427.000000. Reward 36.000000. action: 1.000000. mean reward 45.460705.\n",
      "World Perf: Episode 430.000000. Reward 47.000000. action: 0.000000. mean reward 48.403057.\n",
      "World Perf: Episode 433.000000. Reward 41.333333. action: 0.000000. mean reward 48.651257.\n",
      "World Perf: Episode 436.000000. Reward 41.666667. action: 0.000000. mean reward 48.268505.\n",
      "World Perf: Episode 439.000000. Reward 76.333333. action: 1.000000. mean reward 52.056366.\n",
      "World Perf: Episode 442.000000. Reward 25.333333. action: 1.000000. mean reward 51.788998.\n",
      "World Perf: Episode 445.000000. Reward 76.000000. action: 1.000000. mean reward 51.629963.\n",
      "World Perf: Episode 448.000000. Reward 34.000000. action: 1.000000. mean reward 54.657131.\n",
      "World Perf: Episode 451.000000. Reward 19.000000. action: 1.000000. mean reward 53.956577.\n",
      "World Perf: Episode 454.000000. Reward 44.333333. action: 1.000000. mean reward 53.618679.\n",
      "World Perf: Episode 457.000000. Reward 30.333333. action: 1.000000. mean reward 53.084896.\n",
      "World Perf: Episode 460.000000. Reward 53.000000. action: 0.000000. mean reward 52.683941.\n",
      "World Perf: Episode 463.000000. Reward 42.333333. action: 0.000000. mean reward 52.152355.\n",
      "World Perf: Episode 466.000000. Reward 28.333333. action: 0.000000. mean reward 51.655350.\n",
      "World Perf: Episode 469.000000. Reward 63.000000. action: 1.000000. mean reward 51.449955.\n",
      "World Perf: Episode 472.000000. Reward 21.000000. action: 1.000000. mean reward 51.423534.\n",
      "World Perf: Episode 475.000000. Reward 43.000000. action: 1.000000. mean reward 51.490452.\n",
      "World Perf: Episode 478.000000. Reward 46.000000. action: 0.000000. mean reward 53.027313.\n",
      "World Perf: Episode 481.000000. Reward 21.000000. action: 1.000000. mean reward 52.918552.\n",
      "World Perf: Episode 484.000000. Reward 40.666667. action: 0.000000. mean reward 52.358810.\n",
      "World Perf: Episode 487.000000. Reward 43.666667. action: 1.000000. mean reward 54.667004.\n",
      "World Perf: Episode 490.000000. Reward 63.000000. action: 1.000000. mean reward 54.450317.\n",
      "World Perf: Episode 493.000000. Reward 56.000000. action: 0.000000. mean reward 54.091919.\n",
      "World Perf: Episode 496.000000. Reward 49.000000. action: 0.000000. mean reward 55.373093.\n",
      "World Perf: Episode 499.000000. Reward 53.000000. action: 1.000000. mean reward 54.959309.\n",
      "World Perf: Episode 502.000000. Reward 59.000000. action: 0.000000. mean reward 54.721039.\n",
      "World Perf: Episode 505.000000. Reward 37.333333. action: 0.000000. mean reward 56.817791.\n",
      "World Perf: Episode 508.000000. Reward 72.333333. action: 0.000000. mean reward 57.842865.\n",
      "World Perf: Episode 511.000000. Reward 62.000000. action: 0.000000. mean reward 60.380966.\n",
      "World Perf: Episode 514.000000. Reward 52.666667. action: 0.000000. mean reward 60.245144.\n",
      "World Perf: Episode 517.000000. Reward 45.333333. action: 1.000000. mean reward 59.917984.\n",
      "World Perf: Episode 520.000000. Reward 31.666667. action: 0.000000. mean reward 62.177990.\n",
      "World Perf: Episode 523.000000. Reward 64.333333. action: 1.000000. mean reward 62.012070.\n",
      "World Perf: Episode 526.000000. Reward 27.000000. action: 1.000000. mean reward 61.326630.\n",
      "World Perf: Episode 529.000000. Reward 72.000000. action: 0.000000. mean reward 61.051388.\n",
      "World Perf: Episode 532.000000. Reward 53.000000. action: 1.000000. mean reward 61.110870.\n",
      "World Perf: Episode 535.000000. Reward 57.333333. action: 1.000000. mean reward 61.071884.\n",
      "World Perf: Episode 538.000000. Reward 43.333333. action: 1.000000. mean reward 60.890636.\n",
      "World Perf: Episode 541.000000. Reward 41.333333. action: 1.000000. mean reward 61.355846.\n",
      "World Perf: Episode 544.000000. Reward 69.333333. action: 1.000000. mean reward 62.861889.\n",
      "World Perf: Episode 547.000000. Reward 62.000000. action: 1.000000. mean reward 65.630318.\n",
      "World Perf: Episode 550.000000. Reward 49.666667. action: 0.000000. mean reward 66.794228.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 553.000000. Reward 81.333333. action: 0.000000. mean reward 70.281563.\n",
      "World Perf: Episode 556.000000. Reward 33.666667. action: 0.000000. mean reward 72.187431.\n",
      "World Perf: Episode 559.000000. Reward 54.000000. action: 1.000000. mean reward 71.495659.\n",
      "World Perf: Episode 562.000000. Reward 74.333333. action: 0.000000. mean reward 70.945587.\n",
      "World Perf: Episode 565.000000. Reward 64.666667. action: 1.000000. mean reward 70.455566.\n",
      "World Perf: Episode 568.000000. Reward 41.333333. action: 0.000000. mean reward 69.786598.\n",
      "World Perf: Episode 571.000000. Reward 122.000000. action: 1.000000. mean reward 69.913864.\n",
      "World Perf: Episode 574.000000. Reward 44.000000. action: 1.000000. mean reward 70.977654.\n",
      "World Perf: Episode 577.000000. Reward 77.333333. action: 1.000000. mean reward 70.551750.\n",
      "World Perf: Episode 580.000000. Reward 53.000000. action: 0.000000. mean reward 70.019798.\n",
      "World Perf: Episode 583.000000. Reward 62.333333. action: 1.000000. mean reward 70.308212.\n",
      "World Perf: Episode 586.000000. Reward 74.666667. action: 0.000000. mean reward 70.981430.\n",
      "World Perf: Episode 589.000000. Reward 62.333333. action: 0.000000. mean reward 71.450233.\n",
      "World Perf: Episode 592.000000. Reward 18.666667. action: 1.000000. mean reward 73.729774.\n",
      "World Perf: Episode 595.000000. Reward 45.333333. action: 1.000000. mean reward 74.287148.\n",
      "World Perf: Episode 598.000000. Reward 60.666667. action: 1.000000. mean reward 73.923988.\n",
      "World Perf: Episode 601.000000. Reward 53.333333. action: 0.000000. mean reward 74.422287.\n",
      "World Perf: Episode 604.000000. Reward 84.666667. action: 1.000000. mean reward 77.904648.\n",
      "World Perf: Episode 607.000000. Reward 39.666667. action: 1.000000. mean reward 76.979004.\n",
      "World Perf: Episode 610.000000. Reward 38.666667. action: 1.000000. mean reward 77.235191.\n",
      "World Perf: Episode 613.000000. Reward 70.333333. action: 1.000000. mean reward 76.869728.\n",
      "World Perf: Episode 616.000000. Reward 60.666667. action: 1.000000. mean reward 76.167183.\n",
      "World Perf: Episode 619.000000. Reward 55.000000. action: 1.000000. mean reward 75.366821.\n",
      "World Perf: Episode 622.000000. Reward 42.333333. action: 1.000000. mean reward 74.606873.\n",
      "World Perf: Episode 625.000000. Reward 70.333333. action: 0.000000. mean reward 74.499466.\n",
      "World Perf: Episode 628.000000. Reward 71.666667. action: 1.000000. mean reward 73.855263.\n",
      "World Perf: Episode 631.000000. Reward 86.333333. action: 0.000000. mean reward 73.481491.\n",
      "World Perf: Episode 634.000000. Reward 60.666667. action: 1.000000. mean reward 75.751244.\n",
      "World Perf: Episode 637.000000. Reward 103.000000. action: 1.000000. mean reward 76.869720.\n",
      "World Perf: Episode 640.000000. Reward 44.333333. action: 0.000000. mean reward 75.875572.\n",
      "World Perf: Episode 643.000000. Reward 86.666667. action: 0.000000. mean reward 78.151077.\n",
      "World Perf: Episode 646.000000. Reward 59.666667. action: 1.000000. mean reward 79.558151.\n",
      "World Perf: Episode 649.000000. Reward 90.000000. action: 1.000000. mean reward 79.098022.\n",
      "World Perf: Episode 652.000000. Reward 43.333333. action: 1.000000. mean reward 80.080635.\n",
      "World Perf: Episode 655.000000. Reward 31.333333. action: 0.000000. mean reward 79.563179.\n",
      "World Perf: Episode 658.000000. Reward 65.000000. action: 1.000000. mean reward 79.672417.\n",
      "World Perf: Episode 661.000000. Reward 49.666667. action: 1.000000. mean reward 81.077515.\n",
      "World Perf: Episode 664.000000. Reward 64.333333. action: 0.000000. mean reward 80.298843.\n",
      "World Perf: Episode 667.000000. Reward 82.000000. action: 1.000000. mean reward 80.285271.\n",
      "World Perf: Episode 670.000000. Reward 23.333333. action: 1.000000. mean reward 79.209961.\n",
      "World Perf: Episode 673.000000. Reward 86.333333. action: 1.000000. mean reward 79.105415.\n",
      "World Perf: Episode 676.000000. Reward 42.333333. action: 1.000000. mean reward 78.174408.\n",
      "World Perf: Episode 679.000000. Reward 129.666667. action: 1.000000. mean reward 78.235451.\n",
      "World Perf: Episode 682.000000. Reward 107.000000. action: 0.000000. mean reward 78.233124.\n",
      "World Perf: Episode 685.000000. Reward 31.333333. action: 1.000000. mean reward 77.210533.\n",
      "World Perf: Episode 688.000000. Reward 56.333333. action: 1.000000. mean reward 76.418739.\n",
      "World Perf: Episode 691.000000. Reward 41.333333. action: 1.000000. mean reward 76.272972.\n",
      "World Perf: Episode 694.000000. Reward 101.000000. action: 0.000000. mean reward 76.043571.\n",
      "World Perf: Episode 697.000000. Reward 63.000000. action: 1.000000. mean reward 75.561745.\n",
      "World Perf: Episode 700.000000. Reward 107.666667. action: 1.000000. mean reward 75.779106.\n",
      "World Perf: Episode 703.000000. Reward 64.666667. action: 1.000000. mean reward 77.723610.\n",
      "World Perf: Episode 706.000000. Reward 54.666667. action: 1.000000. mean reward 76.898232.\n",
      "World Perf: Episode 709.000000. Reward 68.000000. action: 1.000000. mean reward 76.558815.\n",
      "World Perf: Episode 712.000000. Reward 92.333333. action: 0.000000. mean reward 76.851891.\n",
      "World Perf: Episode 715.000000. Reward 67.333333. action: 1.000000. mean reward 76.075027.\n",
      "World Perf: Episode 718.000000. Reward 62.000000. action: 0.000000. mean reward 76.292953.\n",
      "World Perf: Episode 721.000000. Reward 82.000000. action: 0.000000. mean reward 75.719856.\n",
      "World Perf: Episode 724.000000. Reward 115.666667. action: 0.000000. mean reward 75.517433.\n",
      "World Perf: Episode 727.000000. Reward 146.000000. action: 1.000000. mean reward 75.644371.\n",
      "World Perf: Episode 730.000000. Reward 50.666667. action: 1.000000. mean reward 74.813400.\n",
      "World Perf: Episode 733.000000. Reward 78.666667. action: 1.000000. mean reward 74.243530.\n",
      "World Perf: Episode 736.000000. Reward 94.666667. action: 1.000000. mean reward 74.164078.\n",
      "World Perf: Episode 739.000000. Reward 94.666667. action: 1.000000. mean reward 76.172798.\n",
      "World Perf: Episode 742.000000. Reward 75.333333. action: 1.000000. mean reward 76.233688.\n",
      "World Perf: Episode 745.000000. Reward 118.000000. action: 1.000000. mean reward 76.372093.\n",
      "World Perf: Episode 748.000000. Reward 96.333333. action: 0.000000. mean reward 75.954025.\n",
      "World Perf: Episode 751.000000. Reward 82.000000. action: 1.000000. mean reward 75.337105.\n",
      "World Perf: Episode 754.000000. Reward 73.666667. action: 0.000000. mean reward 74.618095.\n",
      "World Perf: Episode 757.000000. Reward 91.333333. action: 1.000000. mean reward 74.168694.\n",
      "World Perf: Episode 760.000000. Reward 91.000000. action: 1.000000. mean reward 73.833809.\n",
      "World Perf: Episode 763.000000. Reward 104.666667. action: 0.000000. mean reward 73.708839.\n",
      "World Perf: Episode 766.000000. Reward 52.000000. action: 1.000000. mean reward 73.463326.\n",
      "World Perf: Episode 769.000000. Reward 77.333333. action: 1.000000. mean reward 75.750343.\n",
      "World Perf: Episode 772.000000. Reward 96.000000. action: 0.000000. mean reward 75.289436.\n",
      "World Perf: Episode 775.000000. Reward 75.333333. action: 1.000000. mean reward 74.718575.\n",
      "World Perf: Episode 778.000000. Reward 98.333333. action: 1.000000. mean reward 76.818375.\n",
      "World Perf: Episode 781.000000. Reward 106.666667. action: 1.000000. mean reward 76.487846.\n",
      "World Perf: Episode 784.000000. Reward 128.666667. action: 0.000000. mean reward 76.573021.\n",
      "World Perf: Episode 787.000000. Reward 27.000000. action: 1.000000. mean reward 75.437027.\n",
      "World Perf: Episode 790.000000. Reward 66.333333. action: 1.000000. mean reward 74.790230.\n",
      "World Perf: Episode 793.000000. Reward 132.666667. action: 1.000000. mean reward 76.028801.\n",
      "World Perf: Episode 796.000000. Reward 141.666667. action: 0.000000. mean reward 76.432564.\n",
      "World Perf: Episode 799.000000. Reward 137.666667. action: 1.000000. mean reward 76.419792.\n",
      "World Perf: Episode 802.000000. Reward 152.666667. action: 1.000000. mean reward 79.505028.\n",
      "World Perf: Episode 805.000000. Reward 83.333333. action: 1.000000. mean reward 78.815453.\n",
      "World Perf: Episode 808.000000. Reward 94.000000. action: 1.000000. mean reward 81.036728.\n",
      "World Perf: Episode 811.000000. Reward 129.333333. action: 0.000000. mean reward 81.076485.\n",
      "World Perf: Episode 814.000000. Reward 133.333333. action: 1.000000. mean reward 80.900093.\n",
      "World Perf: Episode 817.000000. Reward 139.000000. action: 1.000000. mean reward 83.502350.\n",
      "World Perf: Episode 820.000000. Reward 126.666667. action: 1.000000. mean reward 86.142998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 823.000000. Reward 92.333333. action: 1.000000. mean reward 85.813316.\n",
      "World Perf: Episode 826.000000. Reward 55.333333. action: 0.000000. mean reward 87.684319.\n",
      "World Perf: Episode 829.000000. Reward 162.333333. action: 0.000000. mean reward 89.883301.\n",
      "World Perf: Episode 832.000000. Reward 99.666667. action: 1.000000. mean reward 91.922523.\n",
      "World Perf: Episode 835.000000. Reward 99.333333. action: 1.000000. mean reward 93.959229.\n",
      "World Perf: Episode 838.000000. Reward 129.333333. action: 0.000000. mean reward 93.540123.\n",
      "World Perf: Episode 841.000000. Reward 121.000000. action: 0.000000. mean reward 93.164848.\n",
      "World Perf: Episode 844.000000. Reward 100.333333. action: 1.000000. mean reward 92.798058.\n",
      "World Perf: Episode 847.000000. Reward 138.333333. action: 0.000000. mean reward 92.679405.\n",
      "World Perf: Episode 850.000000. Reward 156.000000. action: 0.000000. mean reward 92.837669.\n",
      "World Perf: Episode 853.000000. Reward 124.666667. action: 1.000000. mean reward 93.099396.\n",
      "World Perf: Episode 856.000000. Reward 62.333333. action: 1.000000. mean reward 94.811516.\n",
      "World Perf: Episode 859.000000. Reward 134.000000. action: 1.000000. mean reward 97.301117.\n",
      "World Perf: Episode 862.000000. Reward 120.333333. action: 1.000000. mean reward 98.892113.\n",
      "World Perf: Episode 865.000000. Reward 161.666667. action: 1.000000. mean reward 98.658653.\n",
      "World Perf: Episode 868.000000. Reward 112.666667. action: 1.000000. mean reward 99.204414.\n",
      "World Perf: Episode 871.000000. Reward 121.000000. action: 1.000000. mean reward 99.474983.\n",
      "World Perf: Episode 874.000000. Reward 113.333333. action: 0.000000. mean reward 98.722954.\n",
      "World Perf: Episode 877.000000. Reward 127.666667. action: 1.000000. mean reward 98.131737.\n",
      "World Perf: Episode 880.000000. Reward 116.333333. action: 1.000000. mean reward 97.566139.\n",
      "World Perf: Episode 883.000000. Reward 118.666667. action: 1.000000. mean reward 97.506508.\n",
      "World Perf: Episode 886.000000. Reward 105.333333. action: 1.000000. mean reward 99.159019.\n",
      "World Perf: Episode 889.000000. Reward 173.333333. action: 0.000000. mean reward 101.328972.\n",
      "World Perf: Episode 892.000000. Reward 146.000000. action: 1.000000. mean reward 100.898079.\n",
      "World Perf: Episode 895.000000. Reward 136.000000. action: 1.000000. mean reward 100.784843.\n",
      "World Perf: Episode 898.000000. Reward 122.666667. action: 0.000000. mean reward 100.751015.\n",
      "World Perf: Episode 901.000000. Reward 179.666667. action: 1.000000. mean reward 100.898285.\n",
      "World Perf: Episode 904.000000. Reward 104.000000. action: 1.000000. mean reward 100.201088.\n",
      "World Perf: Episode 907.000000. Reward 161.666667. action: 1.000000. mean reward 100.222816.\n",
      "World Perf: Episode 910.000000. Reward 200.000000. action: 0.000000. mean reward 100.758781.\n",
      "World Perf: Episode 913.000000. Reward 185.000000. action: 0.000000. mean reward 102.518791.\n",
      "World Perf: Episode 916.000000. Reward 197.333333. action: 0.000000. mean reward 102.574776.\n",
      "World Perf: Episode 919.000000. Reward 186.333333. action: 1.000000. mean reward 102.565216.\n",
      "World Perf: Episode 922.000000. Reward 182.666667. action: 1.000000. mean reward 105.589447.\n",
      "World Perf: Episode 925.000000. Reward 174.666667. action: 1.000000. mean reward 105.508698.\n",
      "World Perf: Episode 928.000000. Reward 161.333333. action: 1.000000. mean reward 107.321175.\n",
      "World Perf: Episode 931.000000. Reward 197.666667. action: 1.000000. mean reward 110.547691.\n",
      "World Perf: Episode 934.000000. Reward 152.000000. action: 1.000000. mean reward 111.891045.\n",
      "World Perf: Episode 937.000000. Reward 171.000000. action: 0.000000. mean reward 112.280388.\n",
      "World Perf: Episode 940.000000. Reward 169.333333. action: 0.000000. mean reward 111.999855.\n",
      "World Perf: Episode 943.000000. Reward 183.333333. action: 0.000000. mean reward 113.339973.\n",
      "World Perf: Episode 946.000000. Reward 197.666667. action: 1.000000. mean reward 113.273872.\n",
      "World Perf: Episode 949.000000. Reward 190.333333. action: 1.000000. mean reward 116.535805.\n",
      "World Perf: Episode 952.000000. Reward 185.666667. action: 0.000000. mean reward 119.670113.\n",
      "World Perf: Episode 955.000000. Reward 182.666667. action: 1.000000. mean reward 120.469368.\n",
      "World Perf: Episode 958.000000. Reward 200.000000. action: 1.000000. mean reward 123.056984.\n",
      "World Perf: Episode 961.000000. Reward 177.000000. action: 1.000000. mean reward 123.541267.\n",
      "World Perf: Episode 964.000000. Reward 200.000000. action: 1.000000. mean reward 123.318932.\n",
      "World Perf: Episode 967.000000. Reward 200.000000. action: 0.000000. mean reward 123.045937.\n",
      "World Perf: Episode 970.000000. Reward 167.000000. action: 1.000000. mean reward 122.358330.\n",
      "World Perf: Episode 973.000000. Reward 195.333333. action: 1.000000. mean reward 121.986115.\n",
      "World Perf: Episode 976.000000. Reward 194.333333. action: 0.000000. mean reward 121.780113.\n",
      "World Perf: Episode 979.000000. Reward 143.333333. action: 1.000000. mean reward 123.641869.\n",
      "World Perf: Episode 982.000000. Reward 200.000000. action: 1.000000. mean reward 124.391167.\n",
      "World Perf: Episode 985.000000. Reward 200.000000. action: 1.000000. mean reward 125.501366.\n",
      "World Perf: Episode 988.000000. Reward 178.666667. action: 1.000000. mean reward 128.044907.\n",
      "World Perf: Episode 991.000000. Reward 157.000000. action: 1.000000. mean reward 129.652054.\n",
      "World Perf: Episode 994.000000. Reward 200.000000. action: 0.000000. mean reward 131.860794.\n",
      "World Perf: Episode 997.000000. Reward 176.000000. action: 1.000000. mean reward 132.984344.\n",
      "World Perf: Episode 1000.000000. Reward 200.000000. action: 0.000000. mean reward 132.705551.\n",
      "World Perf: Episode 1003.000000. Reward 190.666667. action: 1.000000. mean reward 134.935104.\n",
      "World Perf: Episode 1006.000000. Reward 200.000000. action: 0.000000. mean reward 137.252945.\n",
      "World Perf: Episode 1009.000000. Reward 133.666667. action: 0.000000. mean reward 138.058823.\n",
      "World Perf: Episode 1012.000000. Reward 200.000000. action: 1.000000. mean reward 140.154373.\n",
      "World Perf: Episode 1015.000000. Reward 200.000000. action: 1.000000. mean reward 142.510727.\n",
      "World Perf: Episode 1018.000000. Reward 184.000000. action: 1.000000. mean reward 141.897827.\n",
      "World Perf: Episode 1021.000000. Reward 200.000000. action: 1.000000. mean reward 143.973648.\n",
      "World Perf: Episode 1024.000000. Reward 200.000000. action: 0.000000. mean reward 143.570908.\n",
      "World Perf: Episode 1027.000000. Reward 200.000000. action: 1.000000. mean reward 144.801529.\n",
      "World Perf: Episode 1030.000000. Reward 200.000000. action: 1.000000. mean reward 146.947586.\n",
      "World Perf: Episode 1033.000000. Reward 200.000000. action: 1.000000. mean reward 148.969208.\n",
      "World Perf: Episode 1036.000000. Reward 200.000000. action: 0.000000. mean reward 150.992310.\n",
      "World Perf: Episode 1039.000000. Reward 200.000000. action: 1.000000. mean reward 153.022736.\n",
      "World Perf: Episode 1042.000000. Reward 200.000000. action: 0.000000. mean reward 154.923523.\n",
      "World Perf: Episode 1045.000000. Reward 194.333333. action: 0.000000. mean reward 155.323257.\n",
      "World Perf: Episode 1048.000000. Reward 200.000000. action: 1.000000. mean reward 157.249252.\n",
      "World Perf: Episode 1051.000000. Reward 200.000000. action: 1.000000. mean reward 157.757355.\n",
      "World Perf: Episode 1054.000000. Reward 200.000000. action: 0.000000. mean reward 158.679367.\n",
      "World Perf: Episode 1057.000000. Reward 200.000000. action: 0.000000. mean reward 160.025009.\n",
      "World Perf: Episode 1060.000000. Reward 176.333333. action: 1.000000. mean reward 161.264771.\n",
      "World Perf: Episode 1063.000000. Reward 200.000000. action: 1.000000. mean reward 161.745987.\n",
      "World Perf: Episode 1066.000000. Reward 200.000000. action: 1.000000. mean reward 163.493668.\n",
      "World Perf: Episode 1069.000000. Reward 200.000000. action: 0.000000. mean reward 164.994888.\n",
      "World Perf: Episode 1072.000000. Reward 200.000000. action: 1.000000. mean reward 165.175995.\n",
      "World Perf: Episode 1075.000000. Reward 200.000000. action: 1.000000. mean reward 164.407425.\n",
      "World Perf: Episode 1078.000000. Reward 200.000000. action: 1.000000. mean reward 163.500320.\n",
      "World Perf: Episode 1081.000000. Reward 200.000000. action: 0.000000. mean reward 163.995468.\n",
      "World Perf: Episode 1084.000000. Reward 200.000000. action: 1.000000. mean reward 163.012268.\n",
      "World Perf: Episode 1087.000000. Reward 200.000000. action: 1.000000. mean reward 162.577347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1090.000000. Reward 200.000000. action: 0.000000. mean reward 163.383606.\n",
      "World Perf: Episode 1093.000000. Reward 200.000000. action: 1.000000. mean reward 163.255844.\n",
      "World Perf: Episode 1096.000000. Reward 200.000000. action: 0.000000. mean reward 164.976578.\n",
      "World Perf: Episode 1099.000000. Reward 161.333333. action: 0.000000. mean reward 164.742157.\n",
      "World Perf: Episode 1102.000000. Reward 193.000000. action: 1.000000. mean reward 164.755783.\n",
      "World Perf: Episode 1105.000000. Reward 177.333333. action: 1.000000. mean reward 165.523331.\n",
      "World Perf: Episode 1108.000000. Reward 200.000000. action: 0.000000. mean reward 166.017624.\n",
      "World Perf: Episode 1111.000000. Reward 200.000000. action: 0.000000. mean reward 167.053146.\n",
      "World Perf: Episode 1114.000000. Reward 200.000000. action: 1.000000. mean reward 170.685654.\n",
      "World Perf: Episode 1117.000000. Reward 200.000000. action: 1.000000. mean reward 178.101562.\n",
      "World Perf: Episode 1120.000000. Reward 134.333333. action: 1.000000. mean reward 175.996460.\n",
      "World Perf: Episode 1123.000000. Reward 200.000000. action: 0.000000. mean reward 175.017563.\n",
      "World Perf: Episode 1126.000000. Reward 200.000000. action: 1.000000. mean reward 178.796280.\n",
      "World Perf: Episode 1129.000000. Reward 200.000000. action: 0.000000. mean reward 179.396011.\n",
      "World Perf: Episode 1132.000000. Reward 200.000000. action: 1.000000. mean reward 180.049789.\n",
      "World Perf: Episode 1135.000000. Reward 200.000000. action: 1.000000. mean reward 181.451233.\n",
      "World Perf: Episode 1138.000000. Reward 200.000000. action: 0.000000. mean reward 182.856674.\n",
      "World Perf: Episode 1141.000000. Reward 200.000000. action: 1.000000. mean reward 183.276978.\n",
      "World Perf: Episode 1144.000000. Reward 200.000000. action: 0.000000. mean reward 184.513443.\n",
      "World Perf: Episode 1147.000000. Reward 200.000000. action: 0.000000. mean reward 185.772095.\n",
      "World Perf: Episode 1150.000000. Reward 200.000000. action: 0.000000. mean reward 187.117111.\n",
      "World Perf: Episode 1153.000000. Reward 200.000000. action: 0.000000. mean reward 185.517838.\n",
      "World Perf: Episode 1156.000000. Reward 200.000000. action: 0.000000. mean reward 186.854599.\n",
      "World Perf: Episode 1159.000000. Reward 200.000000. action: 0.000000. mean reward 188.395920.\n",
      "World Perf: Episode 1162.000000. Reward 200.000000. action: 0.000000. mean reward 189.713821.\n",
      "World Perf: Episode 1165.000000. Reward 200.000000. action: 0.000000. mean reward 191.422424.\n",
      "World Perf: Episode 1168.000000. Reward 200.000000. action: 0.000000. mean reward 191.634720.\n",
      "World Perf: Episode 1171.000000. Reward 200.000000. action: 0.000000. mean reward 192.107071.\n",
      "World Perf: Episode 1174.000000. Reward 200.000000. action: 0.000000. mean reward 194.849655.\n",
      "World Perf: Episode 1177.000000. Reward 200.000000. action: 1.000000. mean reward 193.137436.\n",
      "World Perf: Episode 1180.000000. Reward 200.000000. action: 1.000000. mean reward 193.779068.\n",
      "World Perf: Episode 1183.000000. Reward 200.000000. action: 1.000000. mean reward 194.939331.\n",
      "World Perf: Episode 1186.000000. Reward 200.000000. action: 1.000000. mean reward 195.993362.\n",
      "World Perf: Episode 1189.000000. Reward 200.000000. action: 1.000000. mean reward 196.223999.\n",
      "World Perf: Episode 1192.000000. Reward 200.000000. action: 1.000000. mean reward 197.323715.\n",
      "World Perf: Episode 1195.000000. Reward 200.000000. action: 0.000000. mean reward 197.458908.\n",
      "World Perf: Episode 1198.000000. Reward 200.000000. action: 0.000000. mean reward 197.521896.\n",
      "World Perf: Episode 1201.000000. Reward 200.000000. action: 1.000000. mean reward 197.846375.\n",
      "World Perf: Episode 1204.000000. Reward 200.000000. action: 0.000000. mean reward 199.177017.\n",
      "World Perf: Episode 1207.000000. Reward 200.000000. action: 1.000000. mean reward 199.634277.\n",
      "World Perf: Episode 1210.000000. Reward 200.000000. action: 1.000000. mean reward 200.654480.\n",
      "World Perf: Episode 1213.000000. Reward 161.333333. action: 1.000000. mean reward 201.273727.\n",
      "World Perf: Episode 1216.000000. Reward 180.333333. action: 0.000000. mean reward 202.024719.\n",
      "World Perf: Episode 1219.000000. Reward 200.000000. action: 1.000000. mean reward 202.929367.\n",
      "World Perf: Episode 1222.000000. Reward 184.666667. action: 0.000000. mean reward 203.721329.\n",
      "World Perf: Episode 1225.000000. Reward 200.000000. action: 1.000000. mean reward 203.520126.\n",
      "World Perf: Episode 1228.000000. Reward 200.000000. action: 0.000000. mean reward 203.368942.\n",
      "World Perf: Episode 1231.000000. Reward 200.000000. action: 1.000000. mean reward 203.081802.\n",
      "World Perf: Episode 1234.000000. Reward 200.000000. action: 0.000000. mean reward 202.271408.\n",
      "World Perf: Episode 1237.000000. Reward 200.000000. action: 0.000000. mean reward 203.197128.\n",
      "World Perf: Episode 1240.000000. Reward 200.000000. action: 0.000000. mean reward 202.437119.\n",
      "World Perf: Episode 1243.000000. Reward 200.000000. action: 0.000000. mean reward 203.371414.\n",
      "World Perf: Episode 1246.000000. Reward 200.000000. action: 0.000000. mean reward 203.463791.\n",
      "World Perf: Episode 1249.000000. Reward 200.000000. action: 1.000000. mean reward 203.591599.\n",
      "World Perf: Episode 1252.000000. Reward 200.000000. action: 1.000000. mean reward 204.492691.\n",
      "World Perf: Episode 1255.000000. Reward 200.000000. action: 0.000000. mean reward 205.448410.\n",
      "World Perf: Episode 1258.000000. Reward 200.000000. action: 0.000000. mean reward 206.542923.\n",
      "World Perf: Episode 1261.000000. Reward 200.000000. action: 1.000000. mean reward 207.355637.\n",
      "World Perf: Episode 1264.000000. Reward 200.000000. action: 0.000000. mean reward 206.192795.\n",
      "World Perf: Episode 1267.000000. Reward 200.000000. action: 1.000000. mean reward 206.382095.\n",
      "World Perf: Episode 1270.000000. Reward 200.000000. action: 1.000000. mean reward 206.073135.\n",
      "World Perf: Episode 1273.000000. Reward 200.000000. action: 0.000000. mean reward 206.940811.\n",
      "World Perf: Episode 1276.000000. Reward 200.000000. action: 1.000000. mean reward 207.722794.\n",
      "World Perf: Episode 1279.000000. Reward 200.000000. action: 0.000000. mean reward 208.535400.\n",
      "World Perf: Episode 1282.000000. Reward 200.000000. action: 0.000000. mean reward 208.853760.\n",
      "World Perf: Episode 1285.000000. Reward 200.000000. action: 0.000000. mean reward 209.607193.\n",
      "World Perf: Episode 1288.000000. Reward 200.000000. action: 1.000000. mean reward 210.351852.\n",
      "World Perf: Episode 1291.000000. Reward 200.000000. action: 1.000000. mean reward 210.242371.\n",
      "World Perf: Episode 1294.000000. Reward 200.000000. action: 0.000000. mean reward 211.038208.\n",
      "World Perf: Episode 1297.000000. Reward 200.000000. action: 1.000000. mean reward 211.715744.\n",
      "World Perf: Episode 1300.000000. Reward 200.000000. action: 1.000000. mean reward 212.466171.\n",
      "World Perf: Episode 1303.000000. Reward 194.000000. action: 1.000000. mean reward 212.152023.\n",
      "World Perf: Episode 1306.000000. Reward 200.000000. action: 1.000000. mean reward 212.747559.\n",
      "World Perf: Episode 1309.000000. Reward 200.000000. action: 1.000000. mean reward 213.478439.\n",
      "World Perf: Episode 1312.000000. Reward 173.000000. action: 1.000000. mean reward 213.038254.\n",
      "World Perf: Episode 1315.000000. Reward 200.000000. action: 0.000000. mean reward 212.960800.\n",
      "World Perf: Episode 1318.000000. Reward 200.000000. action: 1.000000. mean reward 213.711502.\n",
      "World Perf: Episode 1321.000000. Reward 200.000000. action: 1.000000. mean reward 213.710754.\n",
      "World Perf: Episode 1324.000000. Reward 200.000000. action: 0.000000. mean reward 214.023987.\n",
      "World Perf: Episode 1327.000000. Reward 200.000000. action: 1.000000. mean reward 213.865707.\n",
      "World Perf: Episode 1330.000000. Reward 200.000000. action: 1.000000. mean reward 213.761520.\n",
      "World Perf: Episode 1333.000000. Reward 200.000000. action: 1.000000. mean reward 214.486938.\n",
      "World Perf: Episode 1336.000000. Reward 200.000000. action: 0.000000. mean reward 215.159958.\n",
      "World Perf: Episode 1339.000000. Reward 200.000000. action: 0.000000. mean reward 214.365173.\n",
      "World Perf: Episode 1342.000000. Reward 200.000000. action: 0.000000. mean reward 214.285568.\n",
      "World Perf: Episode 1345.000000. Reward 200.000000. action: 0.000000. mean reward 213.717697.\n",
      "World Perf: Episode 1348.000000. Reward 200.000000. action: 0.000000. mean reward 212.723328.\n",
      "World Perf: Episode 1351.000000. Reward 200.000000. action: 0.000000. mean reward 213.489975.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1354.000000. Reward 200.000000. action: 0.000000. mean reward 214.235733.\n",
      "World Perf: Episode 1357.000000. Reward 200.000000. action: 1.000000. mean reward 214.941833.\n",
      "World Perf: Episode 1360.000000. Reward 200.000000. action: 1.000000. mean reward 215.623352.\n",
      "World Perf: Episode 1363.000000. Reward 200.000000. action: 0.000000. mean reward 216.361954.\n",
      "World Perf: Episode 1366.000000. Reward 198.000000. action: 1.000000. mean reward 217.015701.\n",
      "World Perf: Episode 1369.000000. Reward 200.000000. action: 1.000000. mean reward 217.666000.\n",
      "World Perf: Episode 1372.000000. Reward 200.000000. action: 1.000000. mean reward 217.176773.\n",
      "World Perf: Episode 1375.000000. Reward 200.000000. action: 1.000000. mean reward 217.800659.\n",
      "World Perf: Episode 1378.000000. Reward 200.000000. action: 0.000000. mean reward 218.432175.\n",
      "World Perf: Episode 1381.000000. Reward 200.000000. action: 1.000000. mean reward 219.065567.\n",
      "World Perf: Episode 1384.000000. Reward 200.000000. action: 0.000000. mean reward 219.230942.\n",
      "World Perf: Episode 1387.000000. Reward 200.000000. action: 1.000000. mean reward 219.866333.\n",
      "World Perf: Episode 1390.000000. Reward 200.000000. action: 1.000000. mean reward 219.550064.\n",
      "World Perf: Episode 1393.000000. Reward 200.000000. action: 0.000000. mean reward 220.153870.\n",
      "World Perf: Episode 1396.000000. Reward 200.000000. action: 1.000000. mean reward 220.739822.\n",
      "World Perf: Episode 1399.000000. Reward 200.000000. action: 0.000000. mean reward 221.316147.\n",
      "World Perf: Episode 1402.000000. Reward 200.000000. action: 1.000000. mean reward 219.487228.\n",
      "World Perf: Episode 1405.000000. Reward 200.000000. action: 0.000000. mean reward 220.063904.\n",
      "World Perf: Episode 1408.000000. Reward 200.000000. action: 1.000000. mean reward 220.640213.\n",
      "World Perf: Episode 1411.000000. Reward 200.000000. action: 0.000000. mean reward 219.513428.\n",
      "World Perf: Episode 1414.000000. Reward 200.000000. action: 1.000000. mean reward 220.140442.\n",
      "World Perf: Episode 1417.000000. Reward 200.000000. action: 1.000000. mean reward 220.714844.\n",
      "World Perf: Episode 1420.000000. Reward 197.666667. action: 0.000000. mean reward 221.623108.\n",
      "World Perf: Episode 1423.000000. Reward 200.000000. action: 1.000000. mean reward 221.692459.\n",
      "World Perf: Episode 1426.000000. Reward 160.666667. action: 0.000000. mean reward 222.080490.\n",
      "World Perf: Episode 1429.000000. Reward 200.000000. action: 1.000000. mean reward 223.216995.\n",
      "World Perf: Episode 1432.000000. Reward 200.000000. action: 0.000000. mean reward 221.804428.\n",
      "World Perf: Episode 1435.000000. Reward 177.666667. action: 1.000000. mean reward 222.703003.\n",
      "World Perf: Episode 1438.000000. Reward 200.000000. action: 1.000000. mean reward 223.699326.\n",
      "World Perf: Episode 1441.000000. Reward 200.000000. action: 1.000000. mean reward 221.668808.\n",
      "World Perf: Episode 1444.000000. Reward 200.000000. action: 0.000000. mean reward 222.561722.\n",
      "World Perf: Episode 1447.000000. Reward 200.000000. action: 1.000000. mean reward 223.514587.\n",
      "World Perf: Episode 1450.000000. Reward 200.000000. action: 1.000000. mean reward 221.474533.\n",
      "World Perf: Episode 1453.000000. Reward 200.000000. action: 1.000000. mean reward 222.038132.\n",
      "World Perf: Episode 1456.000000. Reward 200.000000. action: 0.000000. mean reward 222.622070.\n",
      "World Perf: Episode 1459.000000. Reward 200.000000. action: 1.000000. mean reward 221.039322.\n",
      "World Perf: Episode 1462.000000. Reward 200.000000. action: 0.000000. mean reward 221.093826.\n",
      "World Perf: Episode 1465.000000. Reward 200.000000. action: 1.000000. mean reward 221.722305.\n",
      "World Perf: Episode 1468.000000. Reward 200.000000. action: 0.000000. mean reward 221.479080.\n",
      "World Perf: Episode 1471.000000. Reward 200.000000. action: 0.000000. mean reward 221.106995.\n",
      "World Perf: Episode 1474.000000. Reward 200.000000. action: 0.000000. mean reward 221.709366.\n",
      "World Perf: Episode 1477.000000. Reward 167.000000. action: 1.000000. mean reward 221.377335.\n",
      "World Perf: Episode 1480.000000. Reward 200.000000. action: 1.000000. mean reward 219.523727.\n",
      "World Perf: Episode 1483.000000. Reward 200.000000. action: 0.000000. mean reward 219.044434.\n",
      "World Perf: Episode 1486.000000. Reward 200.000000. action: 0.000000. mean reward 219.673141.\n",
      "World Perf: Episode 1489.000000. Reward 200.000000. action: 1.000000. mean reward 218.873169.\n",
      "World Perf: Episode 1492.000000. Reward 200.000000. action: 1.000000. mean reward 219.485123.\n",
      "World Perf: Episode 1495.000000. Reward 200.000000. action: 1.000000. mean reward 220.079025.\n",
      "World Perf: Episode 1498.000000. Reward 200.000000. action: 0.000000. mean reward 220.345993.\n",
      "World Perf: Episode 1501.000000. Reward 200.000000. action: 0.000000. mean reward 220.952103.\n",
      "World Perf: Episode 1504.000000. Reward 200.000000. action: 1.000000. mean reward 221.552292.\n",
      "World Perf: Episode 1507.000000. Reward 200.000000. action: 0.000000. mean reward 222.142624.\n",
      "World Perf: Episode 1510.000000. Reward 200.000000. action: 0.000000. mean reward 222.190475.\n",
      "World Perf: Episode 1513.000000. Reward 200.000000. action: 1.000000. mean reward 222.741013.\n",
      "World Perf: Episode 1516.000000. Reward 159.333333. action: 0.000000. mean reward 222.906845.\n",
      "World Perf: Episode 1519.000000. Reward 200.000000. action: 0.000000. mean reward 223.432495.\n",
      "World Perf: Episode 1522.000000. Reward 200.000000. action: 0.000000. mean reward 223.932907.\n",
      "World Perf: Episode 1525.000000. Reward 200.000000. action: 1.000000. mean reward 223.305130.\n",
      "World Perf: Episode 1528.000000. Reward 200.000000. action: 1.000000. mean reward 223.867310.\n",
      "World Perf: Episode 1531.000000. Reward 200.000000. action: 1.000000. mean reward 224.405106.\n",
      "World Perf: Episode 1534.000000. Reward 200.000000. action: 1.000000. mean reward 224.380432.\n",
      "World Perf: Episode 1537.000000. Reward 200.000000. action: 0.000000. mean reward 224.874512.\n",
      "World Perf: Episode 1540.000000. Reward 200.000000. action: 0.000000. mean reward 225.364212.\n",
      "World Perf: Episode 1543.000000. Reward 200.000000. action: 1.000000. mean reward 224.119034.\n",
      "World Perf: Episode 1546.000000. Reward 200.000000. action: 0.000000. mean reward 224.653427.\n",
      "World Perf: Episode 1549.000000. Reward 200.000000. action: 0.000000. mean reward 224.660110.\n",
      "World Perf: Episode 1552.000000. Reward 200.000000. action: 0.000000. mean reward 224.928894.\n",
      "World Perf: Episode 1555.000000. Reward 200.000000. action: 1.000000. mean reward 225.431717.\n",
      "World Perf: Episode 1558.000000. Reward 200.000000. action: 1.000000. mean reward 225.937195.\n",
      "World Perf: Episode 1561.000000. Reward 200.000000. action: 0.000000. mean reward 226.415146.\n",
      "World Perf: Episode 1564.000000. Reward 200.000000. action: 1.000000. mean reward 226.864929.\n",
      "World Perf: Episode 1567.000000. Reward 200.000000. action: 0.000000. mean reward 225.086441.\n",
      "World Perf: Episode 1570.000000. Reward 200.000000. action: 0.000000. mean reward 223.265686.\n",
      "World Perf: Episode 1573.000000. Reward 200.000000. action: 0.000000. mean reward 223.817261.\n",
      "World Perf: Episode 1576.000000. Reward 200.000000. action: 1.000000. mean reward 224.328796.\n",
      "World Perf: Episode 1579.000000. Reward 200.000000. action: 1.000000. mean reward 224.835693.\n",
      "World Perf: Episode 1582.000000. Reward 200.000000. action: 1.000000. mean reward 224.633667.\n",
      "World Perf: Episode 1585.000000. Reward 200.000000. action: 1.000000. mean reward 225.127823.\n",
      "World Perf: Episode 1588.000000. Reward 200.000000. action: 1.000000. mean reward 223.732361.\n",
      "World Perf: Episode 1591.000000. Reward 200.000000. action: 1.000000. mean reward 224.258469.\n",
      "World Perf: Episode 1594.000000. Reward 200.000000. action: 0.000000. mean reward 224.751419.\n",
      "World Perf: Episode 1597.000000. Reward 200.000000. action: 1.000000. mean reward 223.507812.\n",
      "World Perf: Episode 1600.000000. Reward 200.000000. action: 1.000000. mean reward 224.011108.\n",
      "World Perf: Episode 1603.000000. Reward 200.000000. action: 0.000000. mean reward 224.465744.\n",
      "World Perf: Episode 1606.000000. Reward 200.000000. action: 1.000000. mean reward 222.988358.\n",
      "World Perf: Episode 1609.000000. Reward 178.000000. action: 1.000000. mean reward 222.411148.\n",
      "World Perf: Episode 1612.000000. Reward 200.000000. action: 0.000000. mean reward 222.955307.\n",
      "World Perf: Episode 1615.000000. Reward 200.000000. action: 0.000000. mean reward 223.491623.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1618.000000. Reward 200.000000. action: 1.000000. mean reward 223.995911.\n",
      "World Perf: Episode 1621.000000. Reward 200.000000. action: 0.000000. mean reward 224.488220.\n",
      "World Perf: Episode 1624.000000. Reward 200.000000. action: 0.000000. mean reward 224.936691.\n",
      "World Perf: Episode 1627.000000. Reward 200.000000. action: 0.000000. mean reward 225.405441.\n",
      "World Perf: Episode 1630.000000. Reward 200.000000. action: 0.000000. mean reward 225.075134.\n",
      "World Perf: Episode 1633.000000. Reward 200.000000. action: 0.000000. mean reward 225.531357.\n",
      "World Perf: Episode 1636.000000. Reward 200.000000. action: 1.000000. mean reward 225.745163.\n",
      "World Perf: Episode 1639.000000. Reward 200.000000. action: 0.000000. mean reward 226.207809.\n",
      "World Perf: Episode 1642.000000. Reward 200.000000. action: 1.000000. mean reward 226.684814.\n",
      "World Perf: Episode 1645.000000. Reward 200.000000. action: 1.000000. mean reward 227.126404.\n",
      "World Perf: Episode 1648.000000. Reward 200.000000. action: 0.000000. mean reward 227.562683.\n",
      "World Perf: Episode 1651.000000. Reward 200.000000. action: 1.000000. mean reward 228.002029.\n",
      "World Perf: Episode 1654.000000. Reward 200.000000. action: 0.000000. mean reward 227.497253.\n",
      "World Perf: Episode 1657.000000. Reward 200.000000. action: 0.000000. mean reward 227.960953.\n",
      "World Perf: Episode 1660.000000. Reward 200.000000. action: 1.000000. mean reward 228.374130.\n",
      "World Perf: Episode 1663.000000. Reward 200.000000. action: 1.000000. mean reward 227.906082.\n",
      "World Perf: Episode 1666.000000. Reward 200.000000. action: 1.000000. mean reward 228.350525.\n",
      "World Perf: Episode 1669.000000. Reward 200.000000. action: 0.000000. mean reward 228.755814.\n",
      "World Perf: Episode 1672.000000. Reward 200.000000. action: 1.000000. mean reward 228.642624.\n",
      "World Perf: Episode 1675.000000. Reward 200.000000. action: 1.000000. mean reward 229.066208.\n",
      "World Perf: Episode 1678.000000. Reward 200.000000. action: 0.000000. mean reward 228.883133.\n",
      "World Perf: Episode 1681.000000. Reward 200.000000. action: 1.000000. mean reward 229.298630.\n",
      "World Perf: Episode 1684.000000. Reward 200.000000. action: 0.000000. mean reward 228.695084.\n",
      "World Perf: Episode 1687.000000. Reward 200.000000. action: 0.000000. mean reward 229.100403.\n",
      "World Perf: Episode 1690.000000. Reward 200.000000. action: 1.000000. mean reward 229.505356.\n",
      "World Perf: Episode 1693.000000. Reward 200.000000. action: 0.000000. mean reward 229.321289.\n",
      "World Perf: Episode 1696.000000. Reward 200.000000. action: 0.000000. mean reward 229.723892.\n",
      "World Perf: Episode 1699.000000. Reward 200.000000. action: 0.000000. mean reward 229.736572.\n",
      "World Perf: Episode 1702.000000. Reward 200.000000. action: 1.000000. mean reward 230.120529.\n",
      "World Perf: Episode 1705.000000. Reward 200.000000. action: 1.000000. mean reward 230.490967.\n",
      "World Perf: Episode 1708.000000. Reward 200.000000. action: 0.000000. mean reward 230.070938.\n",
      "World Perf: Episode 1711.000000. Reward 200.000000. action: 0.000000. mean reward 230.052383.\n",
      "World Perf: Episode 1714.000000. Reward 200.000000. action: 0.000000. mean reward 230.429214.\n",
      "World Perf: Episode 1717.000000. Reward 200.000000. action: 0.000000. mean reward 230.812912.\n",
      "World Perf: Episode 1720.000000. Reward 200.000000. action: 0.000000. mean reward 231.128296.\n",
      "World Perf: Episode 1723.000000. Reward 200.000000. action: 1.000000. mean reward 231.439743.\n",
      "World Perf: Episode 1726.000000. Reward 200.000000. action: 0.000000. mean reward 231.805420.\n",
      "World Perf: Episode 1729.000000. Reward 200.000000. action: 0.000000. mean reward 232.167557.\n",
      "World Perf: Episode 1732.000000. Reward 200.000000. action: 0.000000. mean reward 232.468063.\n",
      "World Perf: Episode 1735.000000. Reward 200.000000. action: 1.000000. mean reward 232.812729.\n",
      "World Perf: Episode 1738.000000. Reward 200.000000. action: 0.000000. mean reward 233.085251.\n",
      "World Perf: Episode 1741.000000. Reward 200.000000. action: 0.000000. mean reward 232.675232.\n",
      "World Perf: Episode 1744.000000. Reward 200.000000. action: 0.000000. mean reward 232.900101.\n",
      "World Perf: Episode 1747.000000. Reward 200.000000. action: 1.000000. mean reward 233.281601.\n",
      "World Perf: Episode 1750.000000. Reward 200.000000. action: 0.000000. mean reward 233.648453.\n",
      "World Perf: Episode 1753.000000. Reward 200.000000. action: 0.000000. mean reward 234.025986.\n",
      "World Perf: Episode 1756.000000. Reward 200.000000. action: 1.000000. mean reward 234.498474.\n",
      "World Perf: Episode 1759.000000. Reward 200.000000. action: 1.000000. mean reward 234.906799.\n",
      "World Perf: Episode 1762.000000. Reward 200.000000. action: 0.000000. mean reward 235.171753.\n",
      "World Perf: Episode 1765.000000. Reward 200.000000. action: 1.000000. mean reward 235.700729.\n",
      "World Perf: Episode 1768.000000. Reward 200.000000. action: 0.000000. mean reward 236.129959.\n",
      "World Perf: Episode 1771.000000. Reward 200.000000. action: 1.000000. mean reward 234.107956.\n",
      "World Perf: Episode 1774.000000. Reward 200.000000. action: 1.000000. mean reward 234.432358.\n",
      "World Perf: Episode 1777.000000. Reward 200.000000. action: 0.000000. mean reward 234.731857.\n",
      "World Perf: Episode 1780.000000. Reward 200.000000. action: 0.000000. mean reward 235.091980.\n",
      "World Perf: Episode 1783.000000. Reward 200.000000. action: 0.000000. mean reward 235.334305.\n",
      "World Perf: Episode 1786.000000. Reward 200.000000. action: 0.000000. mean reward 234.685867.\n",
      "World Perf: Episode 1789.000000. Reward 200.000000. action: 1.000000. mean reward 234.888123.\n",
      "World Perf: Episode 1792.000000. Reward 200.000000. action: 1.000000. mean reward 235.182190.\n",
      "World Perf: Episode 1795.000000. Reward 200.000000. action: 1.000000. mean reward 235.376175.\n",
      "World Perf: Episode 1798.000000. Reward 200.000000. action: 1.000000. mean reward 235.676559.\n",
      "World Perf: Episode 1801.000000. Reward 200.000000. action: 0.000000. mean reward 235.917282.\n",
      "World Perf: Episode 1804.000000. Reward 200.000000. action: 0.000000. mean reward 236.200546.\n",
      "World Perf: Episode 1807.000000. Reward 200.000000. action: 1.000000. mean reward 236.470810.\n",
      "World Perf: Episode 1810.000000. Reward 200.000000. action: 0.000000. mean reward 236.708496.\n",
      "World Perf: Episode 1813.000000. Reward 200.000000. action: 0.000000. mean reward 236.977478.\n",
      "World Perf: Episode 1816.000000. Reward 200.000000. action: 0.000000. mean reward 237.184830.\n",
      "World Perf: Episode 1819.000000. Reward 200.000000. action: 0.000000. mean reward 236.504990.\n",
      "World Perf: Episode 1822.000000. Reward 200.000000. action: 1.000000. mean reward 236.753357.\n",
      "World Perf: Episode 1825.000000. Reward 200.000000. action: 1.000000. mean reward 237.006699.\n",
      "World Perf: Episode 1828.000000. Reward 200.000000. action: 0.000000. mean reward 237.234299.\n",
      "World Perf: Episode 1831.000000. Reward 200.000000. action: 0.000000. mean reward 237.481003.\n",
      "World Perf: Episode 1834.000000. Reward 200.000000. action: 1.000000. mean reward 237.736816.\n",
      "World Perf: Episode 1837.000000. Reward 200.000000. action: 0.000000. mean reward 237.192139.\n",
      "World Perf: Episode 1840.000000. Reward 200.000000. action: 1.000000. mean reward 237.429382.\n",
      "World Perf: Episode 1843.000000. Reward 200.000000. action: 0.000000. mean reward 237.656082.\n",
      "World Perf: Episode 1846.000000. Reward 200.000000. action: 0.000000. mean reward 237.897461.\n",
      "World Perf: Episode 1849.000000. Reward 200.000000. action: 0.000000. mean reward 238.106812.\n",
      "World Perf: Episode 1852.000000. Reward 200.000000. action: 1.000000. mean reward 238.279617.\n",
      "World Perf: Episode 1855.000000. Reward 200.000000. action: 0.000000. mean reward 238.528015.\n",
      "World Perf: Episode 1858.000000. Reward 200.000000. action: 0.000000. mean reward 237.856186.\n",
      "World Perf: Episode 1861.000000. Reward 200.000000. action: 0.000000. mean reward 238.032089.\n",
      "World Perf: Episode 1864.000000. Reward 200.000000. action: 1.000000. mean reward 238.260178.\n",
      "World Perf: Episode 1867.000000. Reward 200.000000. action: 1.000000. mean reward 238.468018.\n",
      "World Perf: Episode 1870.000000. Reward 200.000000. action: 0.000000. mean reward 238.667862.\n",
      "World Perf: Episode 1873.000000. Reward 200.000000. action: 0.000000. mean reward 238.851486.\n",
      "World Perf: Episode 1876.000000. Reward 200.000000. action: 1.000000. mean reward 239.031174.\n",
      "World Perf: Episode 1879.000000. Reward 200.000000. action: 0.000000. mean reward 239.302811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 1882.000000. Reward 200.000000. action: 0.000000. mean reward 239.503403.\n",
      "World Perf: Episode 1885.000000. Reward 200.000000. action: 0.000000. mean reward 238.828720.\n",
      "World Perf: Episode 1888.000000. Reward 200.000000. action: 0.000000. mean reward 239.028610.\n",
      "World Perf: Episode 1891.000000. Reward 200.000000. action: 0.000000. mean reward 239.217789.\n",
      "World Perf: Episode 1894.000000. Reward 200.000000. action: 1.000000. mean reward 239.426437.\n",
      "World Perf: Episode 1897.000000. Reward 200.000000. action: 1.000000. mean reward 239.641586.\n",
      "World Perf: Episode 1900.000000. Reward 200.000000. action: 1.000000. mean reward 237.238327.\n",
      "World Perf: Episode 1903.000000. Reward 200.000000. action: 1.000000. mean reward 237.508072.\n",
      "World Perf: Episode 1906.000000. Reward 200.000000. action: 0.000000. mean reward 235.921936.\n",
      "World Perf: Episode 1909.000000. Reward 200.000000. action: 0.000000. mean reward 235.823349.\n",
      "World Perf: Episode 1912.000000. Reward 200.000000. action: 1.000000. mean reward 236.155930.\n",
      "World Perf: Episode 1915.000000. Reward 200.000000. action: 0.000000. mean reward 235.462936.\n",
      "World Perf: Episode 1918.000000. Reward 200.000000. action: 1.000000. mean reward 235.746048.\n",
      "World Perf: Episode 1921.000000. Reward 200.000000. action: 1.000000. mean reward 236.033829.\n",
      "World Perf: Episode 1924.000000. Reward 200.000000. action: 0.000000. mean reward 236.143845.\n",
      "World Perf: Episode 1927.000000. Reward 200.000000. action: 1.000000. mean reward 236.027725.\n",
      "World Perf: Episode 1930.000000. Reward 200.000000. action: 1.000000. mean reward 236.312515.\n",
      "World Perf: Episode 1933.000000. Reward 200.000000. action: 1.000000. mean reward 235.896378.\n",
      "World Perf: Episode 1936.000000. Reward 200.000000. action: 1.000000. mean reward 234.439713.\n",
      "World Perf: Episode 1939.000000. Reward 200.000000. action: 0.000000. mean reward 234.269653.\n",
      "World Perf: Episode 1942.000000. Reward 200.000000. action: 0.000000. mean reward 234.583496.\n",
      "World Perf: Episode 1945.000000. Reward 200.000000. action: 0.000000. mean reward 234.849060.\n",
      "World Perf: Episode 1948.000000. Reward 200.000000. action: 1.000000. mean reward 235.140884.\n",
      "World Perf: Episode 1951.000000. Reward 200.000000. action: 1.000000. mean reward 234.695084.\n",
      "World Perf: Episode 1954.000000. Reward 200.000000. action: 1.000000. mean reward 234.929733.\n",
      "World Perf: Episode 1957.000000. Reward 200.000000. action: 1.000000. mean reward 235.177124.\n",
      "World Perf: Episode 1960.000000. Reward 200.000000. action: 1.000000. mean reward 235.501297.\n",
      "World Perf: Episode 1963.000000. Reward 200.000000. action: 1.000000. mean reward 235.879135.\n",
      "World Perf: Episode 1966.000000. Reward 200.000000. action: 1.000000. mean reward 236.095291.\n",
      "World Perf: Episode 1969.000000. Reward 200.000000. action: 0.000000. mean reward 236.698532.\n",
      "World Perf: Episode 1972.000000. Reward 200.000000. action: 1.000000. mean reward 237.659058.\n",
      "World Perf: Episode 1975.000000. Reward 200.000000. action: 1.000000. mean reward 237.809341.\n",
      "World Perf: Episode 1978.000000. Reward 200.000000. action: 1.000000. mean reward 238.919052.\n",
      "World Perf: Episode 1981.000000. Reward 200.000000. action: 1.000000. mean reward 238.355515.\n",
      "World Perf: Episode 1984.000000. Reward 200.000000. action: 0.000000. mean reward 238.777573.\n",
      "World Perf: Episode 1987.000000. Reward 200.000000. action: 1.000000. mean reward 238.974243.\n",
      "World Perf: Episode 1990.000000. Reward 200.000000. action: 0.000000. mean reward 239.176147.\n",
      "World Perf: Episode 1993.000000. Reward 200.000000. action: 0.000000. mean reward 239.394165.\n",
      "World Perf: Episode 1996.000000. Reward 200.000000. action: 1.000000. mean reward 239.585999.\n",
      "World Perf: Episode 1999.000000. Reward 200.000000. action: 0.000000. mean reward 239.785629.\n",
      "World Perf: Episode 2002.000000. Reward 200.000000. action: 0.000000. mean reward 239.211349.\n",
      "World Perf: Episode 2005.000000. Reward 200.000000. action: 1.000000. mean reward 239.415024.\n",
      "World Perf: Episode 2008.000000. Reward 200.000000. action: 1.000000. mean reward 239.626587.\n",
      "World Perf: Episode 2011.000000. Reward 200.000000. action: 1.000000. mean reward 239.793381.\n",
      "World Perf: Episode 2014.000000. Reward 200.000000. action: 0.000000. mean reward 239.979736.\n",
      "World Perf: Episode 2017.000000. Reward 200.000000. action: 0.000000. mean reward 240.184555.\n",
      "World Perf: Episode 2020.000000. Reward 200.000000. action: 0.000000. mean reward 240.371094.\n",
      "World Perf: Episode 2023.000000. Reward 200.000000. action: 1.000000. mean reward 240.551331.\n",
      "World Perf: Episode 2026.000000. Reward 200.000000. action: 1.000000. mean reward 240.747131.\n",
      "World Perf: Episode 2029.000000. Reward 200.000000. action: 1.000000. mean reward 240.919373.\n",
      "World Perf: Episode 2032.000000. Reward 200.000000. action: 1.000000. mean reward 241.154373.\n",
      "World Perf: Episode 2035.000000. Reward 200.000000. action: 1.000000. mean reward 241.342300.\n",
      "World Perf: Episode 2038.000000. Reward 200.000000. action: 1.000000. mean reward 241.514420.\n",
      "World Perf: Episode 2041.000000. Reward 200.000000. action: 1.000000. mean reward 241.660355.\n",
      "World Perf: Episode 2044.000000. Reward 200.000000. action: 0.000000. mean reward 241.828842.\n",
      "World Perf: Episode 2047.000000. Reward 200.000000. action: 1.000000. mean reward 241.982376.\n",
      "World Perf: Episode 2050.000000. Reward 200.000000. action: 0.000000. mean reward 242.122070.\n",
      "World Perf: Episode 2053.000000. Reward 200.000000. action: 1.000000. mean reward 242.257080.\n",
      "World Perf: Episode 2056.000000. Reward 200.000000. action: 1.000000. mean reward 242.377396.\n",
      "World Perf: Episode 2059.000000. Reward 200.000000. action: 0.000000. mean reward 242.487167.\n",
      "World Perf: Episode 2062.000000. Reward 200.000000. action: 1.000000. mean reward 242.649765.\n",
      "World Perf: Episode 2065.000000. Reward 200.000000. action: 0.000000. mean reward 242.777039.\n",
      "World Perf: Episode 2068.000000. Reward 200.000000. action: 0.000000. mean reward 242.936295.\n",
      "World Perf: Episode 2071.000000. Reward 200.000000. action: 0.000000. mean reward 243.072006.\n",
      "World Perf: Episode 2074.000000. Reward 200.000000. action: 0.000000. mean reward 243.156418.\n",
      "World Perf: Episode 2077.000000. Reward 200.000000. action: 1.000000. mean reward 243.315323.\n",
      "World Perf: Episode 2080.000000. Reward 200.000000. action: 1.000000. mean reward 243.411575.\n",
      "World Perf: Episode 2083.000000. Reward 200.000000. action: 1.000000. mean reward 243.528458.\n",
      "World Perf: Episode 2086.000000. Reward 200.000000. action: 1.000000. mean reward 243.675659.\n",
      "World Perf: Episode 2089.000000. Reward 200.000000. action: 1.000000. mean reward 243.809677.\n",
      "World Perf: Episode 2092.000000. Reward 200.000000. action: 1.000000. mean reward 243.950546.\n",
      "World Perf: Episode 2095.000000. Reward 200.000000. action: 0.000000. mean reward 244.073959.\n",
      "World Perf: Episode 2098.000000. Reward 200.000000. action: 1.000000. mean reward 244.185974.\n",
      "World Perf: Episode 2101.000000. Reward 200.000000. action: 1.000000. mean reward 244.294800.\n",
      "World Perf: Episode 2104.000000. Reward 200.000000. action: 0.000000. mean reward 244.405884.\n",
      "World Perf: Episode 2107.000000. Reward 200.000000. action: 0.000000. mean reward 244.504105.\n",
      "World Perf: Episode 2110.000000. Reward 200.000000. action: 1.000000. mean reward 244.631409.\n",
      "World Perf: Episode 2113.000000. Reward 200.000000. action: 1.000000. mean reward 244.737488.\n",
      "World Perf: Episode 2116.000000. Reward 200.000000. action: 1.000000. mean reward 244.818619.\n",
      "World Perf: Episode 2119.000000. Reward 200.000000. action: 1.000000. mean reward 245.673874.\n",
      "World Perf: Episode 2122.000000. Reward 200.000000. action: 1.000000. mean reward 243.130936.\n",
      "World Perf: Episode 2125.000000. Reward 200.000000. action: 1.000000. mean reward 244.076645.\n",
      "World Perf: Episode 2128.000000. Reward 200.000000. action: 0.000000. mean reward 244.969437.\n",
      "World Perf: Episode 2131.000000. Reward 200.000000. action: 1.000000. mean reward 243.139359.\n",
      "World Perf: Episode 2134.000000. Reward 200.000000. action: 1.000000. mean reward 243.867355.\n",
      "World Perf: Episode 2137.000000. Reward 200.000000. action: 0.000000. mean reward 244.145615.\n",
      "World Perf: Episode 2140.000000. Reward 200.000000. action: 0.000000. mean reward 244.256577.\n",
      "World Perf: Episode 2143.000000. Reward 200.000000. action: 1.000000. mean reward 244.656937.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 2146.000000. Reward 200.000000. action: 0.000000. mean reward 244.760315.\n",
      "World Perf: Episode 2149.000000. Reward 200.000000. action: 1.000000. mean reward 244.850845.\n",
      "World Perf: Episode 2152.000000. Reward 200.000000. action: 0.000000. mean reward 244.952347.\n",
      "World Perf: Episode 2155.000000. Reward 200.000000. action: 0.000000. mean reward 245.045349.\n",
      "World Perf: Episode 2158.000000. Reward 200.000000. action: 0.000000. mean reward 245.181274.\n",
      "World Perf: Episode 2161.000000. Reward 200.000000. action: 0.000000. mean reward 245.288254.\n",
      "World Perf: Episode 2164.000000. Reward 200.000000. action: 0.000000. mean reward 245.381241.\n",
      "World Perf: Episode 2167.000000. Reward 200.000000. action: 0.000000. mean reward 245.473953.\n",
      "World Perf: Episode 2170.000000. Reward 200.000000. action: 1.000000. mean reward 245.564575.\n",
      "World Perf: Episode 2173.000000. Reward 200.000000. action: 1.000000. mean reward 245.643188.\n",
      "World Perf: Episode 2176.000000. Reward 200.000000. action: 1.000000. mean reward 245.737503.\n",
      "World Perf: Episode 2179.000000. Reward 200.000000. action: 1.000000. mean reward 245.819214.\n",
      "World Perf: Episode 2182.000000. Reward 200.000000. action: 0.000000. mean reward 245.895004.\n",
      "World Perf: Episode 2185.000000. Reward 200.000000. action: 0.000000. mean reward 245.991562.\n",
      "World Perf: Episode 2188.000000. Reward 200.000000. action: 1.000000. mean reward 246.295151.\n",
      "World Perf: Episode 2191.000000. Reward 200.000000. action: 1.000000. mean reward 246.391373.\n",
      "World Perf: Episode 2194.000000. Reward 200.000000. action: 0.000000. mean reward 246.460220.\n",
      "World Perf: Episode 2197.000000. Reward 200.000000. action: 1.000000. mean reward 246.575516.\n",
      "World Perf: Episode 2200.000000. Reward 200.000000. action: 1.000000. mean reward 246.766617.\n",
      "World Perf: Episode 2203.000000. Reward 200.000000. action: 1.000000. mean reward 246.834595.\n",
      "World Perf: Episode 2206.000000. Reward 200.000000. action: 0.000000. mean reward 246.976913.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-70817dae536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Start displaying environment once performance is acceptably high.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdrawFromModel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/cartpole.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mglScalef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mglPopMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/gl/lib.pyc\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs,drs,ys,ds = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.initialize_all_variables()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering == True : \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation,[1,4])\n",
    "\n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x) \n",
    "        y = 1 if action == 0 else 0 \n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess,xs,action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        if done: \n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs,drs,ys,ds = [],[],[],[] # reset array memory\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1,:]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:,:]\n",
    "                rewards = np.array(epr[1:,:])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts,rewards,dones])\n",
    "\n",
    "                feed_dict={previous_state: state_prevs, true_observation: state_nexts,true_done:dones,true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss,predicted_state,updateModel],feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix,grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number: \n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                if drawFromModel == False:\n",
    "                    print 'World Perf: Episode %f. Reward %f. action: %f. mean reward %f.' % (real_episodes,reward_sum/real_bs,action, running_reward/real_bs)\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes, we start alternating between training the policy\n",
    "                # from the model and training the model from the real environment.\n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1,0.1,[4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print real_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2*i + 1)\n",
    "    plt.plot(pState[:,i])\n",
    "    plt.subplot(6,2,2*i+1)\n",
    "    plt.plot(state_nextsAll[:,i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are now two network involved, there are plenty of hyper-parameters to adjust in order to improve performance or efficiency. I encourage you to play with them in order to discover better means of combining the the models. In Part 4 I will be exploring how to utilize convolutional networks to learn representations of more complex environments, such as Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9. Deep Q-Networks\n",
    "\n",
    "In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
    "\n",
    "- Going from a single-layer network to a multi-layer convolutional network.\n",
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "It was these three innovations that allowed the [Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent](http://www.davidqiu.com:8888/research/nature14236.pdf). We will be walking through each individual improvement, and showing how to implement it. We won’t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting from Q-Network to Deep Q-Network\n",
    "![DeepMind](https://cdn-images-1.medium.com/max/800/1*T54Ngd-b_CKcP3N6hyXLVg.png)\n",
    "\n",
    "### Addition 1: Convolutional Layers\n",
    "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are [similar to those of the primate visual cortex](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963). As such, they are ideal for the first few elements within our network.\n",
    "\n",
    "In Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:\n",
    "\n",
    "    convolution_layer = tf.contrib.layers.convolution2d(inputs,num_outputs,kernel_size,stride,padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (“VALID”) or add padding around it (“SAME”) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the [Tensorflow documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#convolution2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition 2: Experience Replay\n",
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.\n",
    "\n",
    "### Addition 3: Separate Target Network\n",
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n",
    "\n",
    "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.\n",
    "\n",
    "### Going Beyond DQN\n",
    "With the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture described by DeepMind, have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.\n",
    "\n",
    "### Double DQN\n",
    "The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn’t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n",
    "\n",
    "        Q-Target = r + γQ(s’,argmax(Q(s’,a,ϴ),ϴ’))\n",
    "        \n",
    "### Dueling DQN\n",
    "![Dueling DQN](https://cdn-images-1.medium.com/max/800/1*N_t9I7MeejAoWlDuH1i7cw.png)\n",
    "\n",
    "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n",
    "\n",
    "        Q(s,a) =V(s) + A(a)\n",
    "        \n",
    "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn’t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "![Env](https://cdn-images-1.medium.com/max/800/1*R2nBdO9nJJVY_z0MQO5UsQ.png)\n",
    "\n",
    "Now that we have learned all the tricks to get the most out of our DQN, let’s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine (I am using a GTX970). In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!\n",
    "\n",
    "The game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Actor-Critic Agents (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven’t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.\n",
    "\n",
    "So what is A3C? The [A3C algorithm](https://arxiv.org/pdf/1602.01783.pdf) was released by Google’s DeepMind group earlier this year, and it made a splash by… essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their “universal starter agent” for working with their new (and very diverse) set of Universe environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 As of A3C\n",
    "\n",
    "![Diagram of A3C high-level arcchitecture](https://cdn-images-1.medium.com/max/800/1*YtnGhtSAMnnHSL8PvS7t_w.png)\n",
    "\n",
    "Asynchronous Advantage Actor-Critic is quite a mouthful. Let’s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asynchronous**: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor-Critic**: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantage**: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were “good” and which were “bad.” The network was then updated in order to encourage and discourage actions appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                    Discounted Reward: R = γ(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network’s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:\n",
    "\n",
    "                    Advantage: A = Q(s,a) - V(s)\n",
    "\n",
    "Since we won’t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.\n",
    "\n",
    "                    Advantage Estimate: A = R - V(s)\n",
    "\n",
    "In this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Algorithm\n",
    "![workflow](https://cdn-images-1.medium.com/max/800/1*Hzql_1t0-wwDxiz0C97AcQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by [DennyBritz](https://github.com/dennybritz/reinforcement-learning) and [OpenAI](https://github.com/openai/universe-starter-agent). \n",
    "\n",
    "Both of which I highly recommend if you’d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won’t run on its own. To view and run the full, functional A3C implementation, see this [Github repository](https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general outline of the code architecture is:\n",
    "\n",
    "- **AC_Network** — This class contains all the Tensorflow ops to create the networks themselves.\n",
    "- **Worker** — This class contains a copy of AC_Network, an environment class, as well as all the logic for interacting with the environment, and updating the global network.\n",
    "- High-level code for establishing the Worker instances and running them in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,s_size,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.inputs = tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "            self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])\n",
    "            self.conv1 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.imageIn,num_outputs=16,\n",
    "                kernel_size=[8,8],stride=[4,4],padding='VALID')\n",
    "            self.conv2 = slim.conv2d(activation_fn=tf.nn.elu,\n",
    "                inputs=self.conv1,num_outputs=32,\n",
    "                kernel_size=[4,4],stride=[2,2],padding='VALID')\n",
    "            hidden = slim.fully_connected(slim.flatten(self.conv2),256,activation_fn=tf.nn.elu)\n",
    "            \n",
    "            #Recurrent network for temporal dependencies\n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(256,state_is_tuple=True)\n",
    "            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n",
    "            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n",
    "            self.state_init = [c_init, h_init]\n",
    "            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n",
    "            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n",
    "            self.state_in = (c_in, h_in)\n",
    "            rnn_in = tf.expand_dims(hidden, [0])\n",
    "            step_size = tf.shape(self.imageIn)[:1]\n",
    "            state_in = tf.nn.rnn_cell.LSTMStateTuple(c_in, h_in)\n",
    "            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n",
    "                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n",
    "                time_major=False)\n",
    "            lstm_c, lstm_h = lstm_state\n",
    "            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n",
    "            rnn_out = tf.reshape(lstm_outputs, [-1, 256])\n",
    "            \n",
    "            #Output layers for policy and value estimations\n",
    "            self.policy = slim.fully_connected(rnn_out,a_size,\n",
    "                activation_fn=tf.nn.softmax,\n",
    "                weights_initializer=normalized_columns_initializer(0.01),\n",
    "                biases_initializer=None)\n",
    "            self.value = slim.fully_connected(rnn_out,1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=normalized_columns_initializer(1.0),\n",
    "                biases_initializer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"): \n",
    "    master_network = AC_Network(s_size,a_size,'global',None) # Generate global network\n",
    "    num_workers = multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(Worker(DoomGame(),i,s_size,a_size,trainer,saver,model_path))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print 'Loading Model...'\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # This is where the asynchronous magic happens.\n",
    "    # Start the \"work\" process for each worker in a separate threat.\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(max_episode_length,gamma,master_network,sess,coord)\n",
    "        t = threading.Thread(target=(worker_work))\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ From here we go asynchronous ~\n",
    "\n",
    "Each worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copies one set of variables to another.\n",
    "# Used to set worker network parameters to those of global network.\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "class Worker():\n",
    "    def __init__(self,game,name,s_size,a_size,trainer,saver,model_path):\n",
    "        ....\n",
    "        ....\n",
    "        ....\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_AC = AC_Network(s_size,a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "      ....\n",
    "      ....\n",
    "      ....\n",
    "      def work(self,max_episode_length,gamma,global_AC,sess,coord):\n",
    "        episode_count = 0\n",
    "        total_step_count = 0\n",
    "        print \"Starting worker \" + str(self.number)\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "                d = False\n",
    "                \n",
    "                self.env.new_episode()\n",
    "                s = self.env.get_state().screen_buffer\n",
    "                episode_frames.append(s)\n",
    "                s = process_frame(s)\n",
    "                rnn_state = self.local_AC.state_init\n",
    "                \n",
    "                while self.env.is_episode_finished() == False:\n",
    "                    #Take an action using probabilities from policy network output.\n",
    "                    a_dist,v,rnn_state = sess.run([self.local_AC.policy,self.local_AC.value,self.local_AC.state_out], \n",
    "                        feed_dict={self.local_AC.inputs:[s],\n",
    "                        self.local_AC.state_in[0]:rnn_state[0],\n",
    "                        self.local_AC.state_in[1]:rnn_state[1]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "\n",
    "                    r = self.env.make_action(self.actions[a]) / 100.0\n",
    "                    d = self.env.is_episode_finished()\n",
    "                    if d == False:\n",
    "                        s1 = self.env.get_state().screen_buffer\n",
    "                        episode_frames.append(s1)\n",
    "                        s1 = process_frame(s1)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        \n",
    "                    episode_buffer.append([s,a,r,s1,d,v[0,0]])\n",
    "                    episode_values.append(v[0,0])\n",
    "\n",
    "                    episode_reward += r\n",
    "                    s = s1                    \n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "                    \n",
    "                    #Specific to VizDoom. We sleep the game for a specific time.\n",
    "                    if self.sleep_time>0:\n",
    "                        sleep(self.sleep_time)\n",
    "                    \n",
    "                    # If the episode hasn't ended, but the experience buffer is full, then we\n",
    "                    # make an update step using that experience rollout.\n",
    "                    if len(episode_buffer) == 30 and d != True and episode_step_count != max_episode_length - 1:\n",
    "                        # Since we don't know what the true final return is, we \"bootstrap\" from our current\n",
    "                        # value estimation.\n",
    "                        v1 = sess.run(self.local_AC.value, \n",
    "                            feed_dict={self.local_AC.inputs:[s],\n",
    "                            self.local_AC.state_in[0]:rnn_state[0],\n",
    "                            self.local_AC.state_in[1]:rnn_state[1]})[0,0]\n",
    "                        v_l,p_l,e_l,g_n,v_n = self.train(global_AC,episode_buffer,sess,gamma,v1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if d == True:\n",
    "                        break\n",
    "                                            \n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(np.mean(episode_values))\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                v_l,p_l,e_l,g_n,v_n = self.train(global_AC,episode_buffer,sess,gamma,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the worker’s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Value Loss: L = Σ(R - V(s))²\n",
    "                                        Policy Loss: L = -log(π(s)) * A(s) - β*H(π)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.\n",
    "\n",
    "A worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self,s_size,a_size,scope,trainer):\n",
    "        ....\n",
    "        ....\n",
    "        ....\n",
    "        if scope != 'global':\n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "            self.target_v = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.advantages = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "\n",
    "            self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "            #Loss functions\n",
    "            self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])))\n",
    "            self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n",
    "            self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs)*self.advantages)\n",
    "            self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n",
    "\n",
    "            #Get gradients from local network using local losses\n",
    "            local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "            self.gradients = tf.gradients(self.loss,local_vars)\n",
    "            self.var_norms = tf.global_norm(local_vars)\n",
    "            grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)\n",
    "\n",
    "            #Apply local gradients to global network\n",
    "            global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "            self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))\n",
    "\n",
    "class Worker():\n",
    "      ....\n",
    "      ....\n",
    "      ....\n",
    "      def train(self,global_AC,rollout,sess,gamma,bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:,0]\n",
    "        actions = rollout[:,1]\n",
    "        rewards = rollout[:,2]\n",
    "        next_observations = rollout[:,3]\n",
    "        values = rollout[:,5]\n",
    "\n",
    "        # Here we take the rewards and values from the rollout, and use them to \n",
    "        # generate the advantage and discounted returns. \n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus,gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n",
    "        advantages = discount(advantages,gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.state_init\n",
    "        feed_dict = {self.local_AC.target_v:discounted_rewards,\n",
    "            self.local_AC.inputs:np.vstack(observations),\n",
    "            self.local_AC.actions:actions,\n",
    "            self.local_AC.advantages:advantages,\n",
    "            self.local_AC.state_in[0]:rnn_state[0],\n",
    "            self.local_AC.state_in[1]:rnn_state[1]}\n",
    "        v_l,p_l,e_l,g_n,v_n,_ = sess.run([self.local_AC.value_loss,\n",
    "            self.local_AC.policy_loss,\n",
    "            self.local_AC.entropy,\n",
    "            self.local_AC.grad_norms,\n",
    "            self.local_AC.var_norms,\n",
    "            self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        return v_l / len(rollout),p_l / len(rollout),e_l / len(rollout), g_n,v_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.\n",
    "\n",
    "To view the full and functional code, see the Github repository [here](https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing Doom\n",
    "\n",
    "![Doom](https://cdn-images-1.medium.com/max/800/1*yjPVxRywI8U9SKfCMCKR1A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. \n",
    "\n",
    "VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install vizdoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is installed, we will be using the basic.wad environment, which is provided in the Github repository, and needs to be placed in the working directory.\n",
    "\n",
    "The challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Average reward over time for three workers on Doom task. 0.5 reward corresponds to optimal performance. X-axis represents number of training episodes per worker.](https://cdn-images-1.medium.com/max/800/1*DVaQXq6aVWYsU3S3INkV_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MarioKart\n",
    "\n",
    "The goal of this part is to apply Reinforcement Learning on Mario Kart. The first step deals with single player while the second focuses on using two models for two players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Player\n",
    "\n",
    "#### Running the MarioKart Environement in OpenAI Gym\n",
    "In this section, we will start configuring the OpenAI Gym environement for MarioKart.\n",
    "The MarioKart enviroenment is not an official OpenAI Gym, it is an open source community project that can be found at https://github.com/bzier/gym-mupen64plus\n",
    "\n",
    "![goal](https://cdn-images-1.medium.com/max/800/1*rb8r_CrUM8otagIb4ck_Tw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym, gym_mupen64plus\n",
    "\n",
    "env = gym.make('Mario-Kart-Luigi-Raceway-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "for i in range(88):\n",
    "    (obs, rew, end, info) = env.step([0, 0, 0, 0, 0]) # NOOP until green light\n",
    "    env.render()\n",
    "\n",
    "for i in range(100):\n",
    "    (obs, rew, end, info) = env.step([0, 0, 1, 0, 0]) # Drive straight\n",
    "    env.render()\n",
    "\n",
    "raw_input(\"Press <enter> to exit... \")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you could launch a dummy mariokart for a single player, the idea would be to use A3C to initiate the learning process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Training\n",
    "\n",
    "In the following examples, I use `mario-kart-agent` as the name of my project. You can use your own name instead, or you can remove the switch (`-p`) and project name and they will default to using the directory name.\n",
    "\n",
    "## Start training:\n",
    "After you've completed the build instructions above, starting training is as simple as:\n",
    "```sh\n",
    "docker-compose -p mario-kart-agent up -d\n",
    "```\n",
    "\n",
    "## Scaling workers:\n",
    "If you have sufficient CPU, you can also spin up multiple worker containers that all communicate with the same parameter server container:\n",
    "```sh\n",
    "docker-compose -p mario-kart-agent up -d --scale worker=2\n",
    "```\n",
    "From the original A3C starter agent `README` page, the following recommendation was made. My CPU isn't good enough to even run more than one at a time, so I can't verify if this holds true for Mario Kart as well or not:\n",
    ">For best performance, it is recommended for the number of workers to not exceed available number of CPU cores.\n",
    "\n",
    "## Stop training:\n",
    "When you are done, you can tear down the containers with:\n",
    "```sh\n",
    "docker-compose -p mario-kart-agent down\n",
    "```\n",
    "\n",
    "**`Note 1`**: This will destroy all of the containers, but not the underlying images that you previously built. You can just follow the instructions in [this section](#start-training) to start again.\n",
    "\n",
    "**`Note 2`**: This will ***not*** destroy the `mklogs` volume, which is where all of the log data and model checkpoints are stored. This means that starting the containers again will pick up training essentially where it left off (not mid-episode, but with the parameters of the neural network where they were).\n",
    "\n",
    "## Monitoring training:\n",
    "### Tensorboard (i.e. training data)\n",
    "Tensorboard is started in the parameter server container. This container has the Tensorboard port (`12345`) mapped to the same port on your host. In other words, you can browse to [`http://localhost:12345`](http://localhost:12345) to view the Tensorboard UI.\n",
    "\n",
    "### VNC (i.e. watch the agent)\n",
    "VNC is started in each worker container. Because workers can be scaled out, we can't map the container port to the host machine. Otherwise each of the containers would conflict fighting for the same host port. Instead, you can examine each worker container with:\n",
    "```sh\n",
    "docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}::{{range $p, $conf := .NetworkSettings.Ports}}{{$p}} -> localhost::{{(index $conf 0).HostPort}}{{end}}' $INSTANCE_ID\n",
    "```\n",
    "*Here `$INSTANCE_ID` is the name or ID of the worker container you're interested in*\n",
    "\n",
    "This will return something like:\n",
    "```sh\n",
    "172.17.0.2::5900/tcp -> localhost::32790\n",
    "```\n",
    "Then you can use your favorite VNC client to connect to localhost on the dynamically chosen port and watch the agent in real-time. Note that running the VNC client can cause some performance overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi player\n",
    "Go through the code and update it so that it can now launch the game in multiplayer mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
